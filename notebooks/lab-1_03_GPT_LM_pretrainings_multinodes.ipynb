{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Multi-Node Distributed Training Strategies\n",
    "\n",
    "In this notebook, we will learn how to run [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) GPT pretraing on multiple nodes.\n",
    "\n",
    "\n",
    "## The goals\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Run simple multi-node training of Megatron-LM scripts\n",
    "* Run a hybrid multi-node execution with data, tensor and pipeline parallel distributions\n",
    "\n",
    "\n",
    "**[3.1 Multi-Node Training Execution of Megatron-LM GPT Pretraining](#1.1-The-hardware-overview)<br>**\n",
    "**[3.2 Multi-Node Execution with Data Parallelism](#1.1-The-hardware-overview)<br>**\n",
    "**[3.3 Inter/Intra Node Communications](#1.1-The-hardware-overview)<br>**\n",
    "**[3.4 Profiling](#1.1-Profiling)<br>**\n",
    "**[3.5 Exercise: Hybrid Distributed Training Strategy](#1.1-The-hardware-overview)<br>**\n",
    "**[3.6 Increase The Batchsize / GPU](#1.1-The-hardware-overview)<br>**\n",
    "\n",
    "### Cancel Previous Running/Pending Jobs\n",
    "\n",
    "Before moving on, check that no jobs are still running or waiting on the SLURM queue. Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.1 Multi-Node Training Execution of Megatron-LM GPT Pretraining\n",
    "\n",
    "In the previous notebook, we submitted our jobs in an interactive session after allocating 1 node. \n",
    "\n",
    "For multi-node jobs, we need to rely on the SLURM scheduler. By default, multi-node training with Megatron-LM uses the [NVIDIA Collective Communications Library - NCCL](https://developer.nvidia.com/nccl) distributed backend of the [PyTorch distributed launcher](https://pytorch.org/docs/stable/distributed.html).\n",
    "\n",
    "For 2-Node execution, we will use `SBATCH` scripting. While python execution commands and their arguments remain similar to how it was done for Single Node, we will need additional `SBATCH` arguments for resource allocation. \n",
    "\n",
    "Let's start by executing Megatron-LM GPT pretraining on 2 nodes using only Data Parallelism, meaning that the model is copied on the 4 allocated GPUs, each processing different data batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Multi-Node Execution with Data Parallelism\n",
    "\n",
    "In the previous 2-GPU data parallelism execution, the batch size processed by each GPU was 2 (set by `--micro-batch-size`) corresponding to a global batch size of 4 (set by `--global-batch-size`). \n",
    "\n",
    "When using 4 GPUs, we can keep the micro batch size per GPUs to 2 and set the global batch size to 8 (MICRO_BATCH_SIZE $ \\times $ 4).\n",
    "\n",
    "Let's have a look at the script before allocating resources and executing it. \n",
    "\n",
    "Notice the `SBATCH` arguments for allocating resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at Megaton-LM GPT pretraining execution on 2 nodes\n",
    "!cat /dli/code/pretrain_gpt_2Node4GPU.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the sbatch [script pretrain_gpt_2Node4GPU.sh](./code/pretrain_gpt_2Node4GPU.sh) and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the 2 nodes jobs\n",
    "!sbatch /dli/code/pretrain_gpt_2Node4GPU.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the GPU usage by running the `nvidia-smi` command on the master lab node. After a few seconds, when the nodes are allocated and the script begins its execution, we should see the GPUs 0,1,2,3 utilized, as shown below. Please notice that if you are running Megatron-LM for the first time in the class, the code will need about 6 minutes to be compiled. Until there, you will not be able to see any GPU activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/2N_4gpus_utilization.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU utilization on the master node\n",
    "!sleep 10\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of the Megatron GPT-3 pretraining, we can check the generated [logs](./megatron/logs/log_2Nodes4GPUS.txt) during execution.\n",
    "\n",
    "Let's first verify the world size of our run. We should see this:\n",
    "```\n",
    "using world size: 4, data-parallel-size: 4, tensor-model-parallel size: 1, pipeline-model-parallel size: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"using world size:\" /dli/megatron/logs/log_2Nodes4GPUS.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the performance of the GPT pretraining on 4 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep iteration /dli/megatron/logs/log_2Nodes4GPUS.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the extract logs, we can see the training performance while using 4 GPUs (2 GPUs per node). \n",
    "\n",
    "```\n",
    "iteration      100/     100 | consumed samples:          800 | elapsed time per iteration (ms): 537.3 | learning rate: 5.822E-05 | global batch size:     8 | lm loss: 7.448950E+00 | loss scale: 1.0 | grad norm: 1.303 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
    "```\n",
    "Compare this with 1 Node executions, and discuss it with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.3 Inter/Intra Node Communications \n",
    "\n",
    "In the previous run, you may have noticed that we added the NCCL variable `NCCL_DEBUG=INFO` in order to output the NCCL debug log traces during training as shown bellow. \n",
    "\n",
    "![title](images/nodes_communication.png)\n",
    "\n",
    "\n",
    "This will allow us to check the type of networking used between GPUs and nodes during the training. \n",
    "\n",
    "The direct GPU-to-GPU communication are reposted as `P2P/IPC` while internode communications are reported to be through the `NET/Socket/0`.\n",
    "In our configuration, direct GPU-to-GPU communication should be used within nodes: GPU0<->GPU1 and GPU2<->GPU3). \n",
    "\n",
    "Let's check what NCCL reported in the generated [logs](./megatron/logs/log_2Nodes4GPUS.txt) during the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!grep Channel /dli/megatron/logs/log_2Nodes4GPUS.txt | grep slurmnode1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.4 Monitoring and Profiling the Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, monitoring the training runs with Megatron-LM was done via the text log files. However, monitoring training is also possible through tensorboard visualization of the hyper parameters and training/evaluation metrics. In addition, visualizing can be helpful for debugging and optimizing the models during training.\n",
    "\n",
    "## 3.4.1 Training Metrics Visualization on Tensorboard\n",
    "\n",
    "<img src=\"images/tensorboard1.png\" width=\"950\"/>\n",
    "\n",
    "In the previous Megatron-LM runs, we set the arguments for recording Tensorboard events. The graphs of all the previous experiments are available in the folder `megatron/tensorboard/`.\n",
    "\n",
    "Execute the next cell to create a link to Tensorboard for the browser. Then, click the link to see graphs of experiment metrics saved in the specified `Tensorboard` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Pytorch Profiler with Tensorboard\n",
    "\n",
    "Several existing profiling tools can be used to investigate bottlenecks during the training and inference processes. This allows us to identify the most expensive operations, issues such as GPU starvation, or unnecessary operations. \n",
    "\n",
    "In this class, we will use the [Pytorch Profiler](https://pytorch.org/docs/stable/profiler.html), a tool collecting performance metrics during Pytorch executions. The [Tensorboard-plugin](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html) with the PyTorch Profiler provides a visualization of each GPU process, with several measures of operations running on each GPU, whether using hardware accelerator (TensorCores). It also provides recommendations to improve the process.\n",
    "\n",
    "We will trace only the execution of the GPU_0, monitoring operations during 2 training steps. We specified the profiling by setting `profile-execution` to *True* and providing the `profile-name` to *baseline*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the previous run, profiling is available on the Tensorboard link at the `pytorch_profiler` tab.\n",
    "<img src=\"images/profiling1.png\" width=\"900\"/>\n",
    "\n",
    "In case you already closed the Tensorboard page, you can re-generate the link by executing the next cell. Click the link to open Tensorboard and then, go to the `PYTORCH_PROFILER` tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiler homepage shows an overview of tracing. \n",
    "\n",
    "\n",
    "### The Profiling Overview\n",
    "Several panels are shown on the homepage:\n",
    "- The GPU Summary: shows the GPU configuration and utilization. In our previous run, no TensorCores are used as we have not enabled Mixed Precision training (will see this in the next notebook). \n",
    "\n",
    "- The Step Time Breakdown: shows the execution time spent in each operation. In our previous run,  48.5% of the step time is in communication and 41.7% in Kernel execution time on GPU \n",
    "\n",
    "- Performance Recommendation: Providing recommendations on how to improve the process. There are a few recommendations for our previous run, such as reducing communication cost by using Gradient Accumulation or increasing the batch size. The profiler suggests also enabling Automatic Mixed Precision to speedup Kernels. In the next section, let us first start by increasing the batch size and see its impact in the process (speedup and memory consumption).\n",
    "\n",
    "### The Memory View\n",
    "Let's have a look at the Memory View showing the memory allocation and deallocation of our run. We can zoom into the memory curve to see the related memory events.\n",
    "\n",
    "<img src=\"images/profiling4.png\" width=\"750\"/>\n",
    "\n",
    "The memory allocation followed by deallocation corresponds to the forward and backward pass. This process is traced twice as we traced 2 training steps. We can also see that a peak of ~10GB of the GPU device memory is used during the training step. We can increase the memory usage by increasing the model or the batch size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\\\n",
    "Before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints\n",
    "!rm /dli/megatron/checkpoints/* -r "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.5 Increase The Batchsize / GPU\n",
    "\n",
    "**Special Warning:** In this section, Out Of Memory (OOM) issues are expected! No problem, we will see a few solutions addressing this problem in the next notebook.\n",
    "\n",
    "\n",
    "The size of the current GPT model fits into the GPU memory and thus does not require model distribution (tensor or pipeline parallelism). \\\n",
    "Using only data parallelism, we can improve the training throughput (number of sequences processed per second) by increasing the batch size processed per GPU until reaching the maximum batch size that fits on GPU memory.\\\n",
    "Let's check this by doubling the data processed by each GPU. When using 4 GPUs with only data parallelism (DP_SIZE $= 4$), by increasing the micro batch size per GPUs from 2 to 4, the global batch size should be MICRO_BATCH_SIZE $\\times$ TP_SIZE $= 16$.\n",
    "\n",
    "Let's prepare the training script for this scenario by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_DP_4_MBS_4.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1 \n",
    "\n",
    "# SLURM args\n",
    "MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
    "MASTER_PORT=6000\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=32      # <--- CHANGED HERE\n",
    "GLOBAL_BATCH_SIZE=128    # <--- CHANGED HERE\n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "VOCAB_SIZE=50257\n",
    "\n",
    "# Data Paths\n",
    "DATA_OUTPUT_PATH=/dli/megatron/checkpoints/test\n",
    "CHECKPOINT_PATH=/dli/megatron/checkpoints\n",
    "TENSORBOARD_PATH=/dli/megatron/tensorboard\n",
    "LOGS_PATH=/dli/megatron/logs\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=/dli/data/GPT-2_assets/my-gpt2_text_document\n",
    "\n",
    "NAME=\"log_2Nodes4GPUS_DP_4_MBS_4\"\n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            --optimizer adam \\\n",
    "            --adam-beta1 0.9 \\\n",
    "            --adam-beta2 0.95 \\\n",
    "            --adam-eps 1e-8 \\\n",
    "            --lr 6e-5 \\\n",
    "            --min-lr 6e-6 \\\n",
    "            --lr-decay-style cosine \\\n",
    "            --lr-decay-iters 800 \\\n",
    "            --lr-warmup-fraction .01 \\\n",
    "            --clip-grad 1.0 \\\n",
    "            --weight-decay 1e-1 \\\n",
    "            --exit-duration-in-mins 1190 \\\n",
    "              \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            --num-layers $NLAYERS \\\n",
    "            --hidden-size $NHIDDEN \\\n",
    "            --num-attention-heads $NHEADS \\\n",
    "            --seq-length $SEQ_LEN \\\n",
    "            --max-position-embeddings $SEQ_LEN \\\n",
    "            --micro-batch-size $MICRO_BATCH_SIZE \\\n",
    "            --global-batch-size $GLOBAL_BATCH_SIZE \\\n",
    "            --train-iters 100 \\\n",
    "            --vocab-file $VOCAB_FILE \\\n",
    "            --merge-file $MERGE_FILE \\\n",
    "            --init-method-std 0.006 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "        \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            --log-interval 10 \\\n",
    "            --save-interval 300 \\\n",
    "            --eval-interval 1000 \\\n",
    "            --eval-iters 10 \\\n",
    "            --tensorboard-dir $TENSORBOARD_PATH \\\n",
    "            --tensorboard-queue-size 1 \\\n",
    "            --log-timers-to-tensorboard \\\n",
    "            --log-batch-size-to-tensorboard \\\n",
    "            --log-validation-ppl-to-tensorboard \\\n",
    "            --profile-execution True \\\n",
    "            --profile-name DP_4_MBS_4 \\\n",
    "            \"\n",
    "\n",
    "export LAUNCHER=\"python -u -m torch.distributed.launch \\\n",
    "             --nproc_per_node $GPUS_PER_NODE \\\n",
    "             --nnodes $NNODES \\\n",
    "             --master_addr $MASTER_ADDR \\\n",
    "             --master_port $MASTER_PORT \\\n",
    "             \"\n",
    "\n",
    "export CMD=\" \\\n",
    "             /dli/megatron/Megatron-LM/pretrain_gpt.py \\\n",
    "             --tensor-model-parallel-size $TP_SIZE \\\n",
    "             --pipeline-model-parallel-size $PP_SIZE \\\n",
    "             $GPT_ARGS \\\n",
    "             $OUTPUT_ARGS \\\n",
    "             --save $CHECKPOINT_PATH \\\n",
    "             --data-path $DATA_PATH \\\n",
    "             --data-impl mmap \\\n",
    "             --split 949,50,1 \\\n",
    "             --distributed-backend nccl \\\n",
    "           \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO  $LAUNCHER --node_rank $SLURM_PROCID $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_DP_4_MBS_4.sh](./code/pretrain_gpt_2Node4GPU_DP_4_MBS_4.sh) and check the SLURM queue using the squeue command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the 2 nodes jobs\n",
    "!sbatch /dli/code/pretrain_gpt_2Node4GPU_DP_4_MBS_4.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the Megatron GPT-3 pretraining performance by looking at the generated [logs](./megatron/logs/log_2Nodes4GPUS_hybrid_solution.txt) during the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep iteration /dli/megatron/logs/log_2Nodes4GPUS_DP_4_MBS_4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "No elapsed time per iteration is shown. **What just happened?!** \n",
    "\n",
    "Are we saturating the GPU memory? Run the cell bellow to search for RuntimeError in our logs. You should see:\n",
    "```\n",
    "RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 15.78 GiB total capacity; 13.90 GiB already allocated; 302.00 MiB free; 14.43 GiB reserved in total by PyTorch)\n",
    "RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 1; 15.78 GiB total capacity; 13.90 GiB already allocated; 302.00 MiB free; 14.43 GiB reserved in total by PyTorch)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"RuntimeError\" /dli/megatron/logs/log_2Nodes4GPUS_DP_4_MBS_4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Indeed, we should see `CUDA out of memory` errors which means that the GPU memory cannot handle training this transformer model size with the specified arguments.\n",
    "\n",
    "Doubling the amount of data processed per GPU is too much for the 16G of memory.  \n",
    "\n",
    "In the next lab, we will focus on how to address this issue by reducing the model's memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints and logs directory\n",
    "!rm -rf /dli/megatron/checkpoints/* "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.6 Exercise: Hybrid Distributed Training Strategy \n",
    "\n",
    "\n",
    "Let's configure a new Megatron-LM GPT pretraining execution on 2 nodes (4 GPUs) with both tensor and pipeline parallel by modifying the \"FIXME\" in the following cell. \n",
    "\n",
    "To use tensor and pipeline parallel, we can keep distributed the model using in 2 dimensions: 2 GPUs for tensor parallelism and 2 GPUs for pipeline parallelism. Thus, no more resources will remain for the data parallel. In this case, the `GLOBAL_BATCH_SIZE` should be downgraded to the same size as the `MICRO_BATCH_SIZE`.\n",
    "\n",
    "If you get stuck, view the [solution](solutions/ex3.4.ipynb) for a hint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_hybrid.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=#FIXEME        # <--- CHANGE HERE\n",
    "PP_SIZE=#FIXEME         # <--- CHANGE HERE\n",
    "\n",
    "# SLURM args\n",
    "MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
    "MASTER_PORT=6000\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=#FIXEME          # <--- CHANGE HERE\n",
    "GLOBAL_BATCH_SIZE=#FIXEME         # <--- CHANGE HERE\n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "VOCAB_SIZE=50257\n",
    "\n",
    "# Data Paths\n",
    "DATA_OUTPUT_PATH=/dli/megatron/checkpoints/test\n",
    "CHECKPOINT_PATH=/dli/megatron/checkpoints\n",
    "TENSORBOARD_PATH=/dli/megatron/tensorboard\n",
    "LOGS_PATH=/dli/megatron/logs\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=/dli/data/GPT-2_assets/my-gpt2_text_document\n",
    "\n",
    "NAME=\"log_2Nodes4GPUS_hybrid\"\n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            --optimizer adam \\\n",
    "            --adam-beta1 0.9 \\\n",
    "            --adam-beta2 0.95 \\\n",
    "            --adam-eps 1e-8 \\\n",
    "            --lr 6e-5 \\\n",
    "            --min-lr 6e-6 \\\n",
    "            --lr-decay-style cosine \\\n",
    "            --lr-decay-iters 800 \\\n",
    "            --lr-warmup-fraction .01 \\\n",
    "            --clip-grad 1.0 \\\n",
    "            --weight-decay 1e-1 \\\n",
    "            --exit-duration-in-mins 1190 \\\n",
    "              \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            --num-layers $NLAYERS \\\n",
    "            --hidden-size $NHIDDEN \\\n",
    "            --num-attention-heads $NHEADS \\\n",
    "            --seq-length $SEQ_LEN \\\n",
    "            --max-position-embeddings $SEQ_LEN \\\n",
    "            --micro-batch-size $MICRO_BATCH_SIZE \\\n",
    "            --global-batch-size $GLOBAL_BATCH_SIZE \\\n",
    "            --train-iters 100 \\\n",
    "            --vocab-file $VOCAB_FILE \\\n",
    "            --merge-file $MERGE_FILE \\\n",
    "            --init-method-std 0.006 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "        \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            --log-interval 10 \\\n",
    "            --save-interval 300 \\\n",
    "            --eval-interval 1000 \\\n",
    "            --eval-iters 10 \\\n",
    "            --tensorboard-dir $TENSORBOARD_PATH \\\n",
    "            --tensorboard-queue-size 1 \\\n",
    "            --log-timers-to-tensorboard \\\n",
    "            --log-batch-size-to-tensorboard \\\n",
    "            --log-validation-ppl-to-tensorboard \\\n",
    "            --profile-execution True \\\n",
    "            --profile-name TP_PP \\\n",
    "            \"\n",
    "\n",
    "export LAUNCHER=\"python -u -m torch.distributed.launch \\\n",
    "             --nproc_per_node $GPUS_PER_NODE \\\n",
    "             --nnodes $NNODES \\\n",
    "             --master_addr $MASTER_ADDR \\\n",
    "             --master_port $MASTER_PORT \\\n",
    "             \"\n",
    "\n",
    "export CMD=\" \\\n",
    "             /dli/megatron/Megatron-LM/pretrain_gpt.py \\\n",
    "             --tensor-model-parallel-size $TP_SIZE \\\n",
    "             --pipeline-model-parallel-size $PP_SIZE \\\n",
    "             $GPT_ARGS \\\n",
    "             $OUTPUT_ARGS \\\n",
    "             --save $CHECKPOINT_PATH \\\n",
    "             --data-path $DATA_PATH \\\n",
    "             --data-impl mmap \\\n",
    "             --split 949,50,1 \\\n",
    "             --distributed-backend nccl \\\n",
    "           \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO  $LAUNCHER --node_rank $SLURM_PROCID $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_hybrid.sh](/dli/code/pretrain_gpt_2Node4GPU_hybrid.sh) for hybrid multi-node run and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the 2 nodes jobs\n",
    "!sbatch /dli/code/pretrain_gpt_2Node4GPU_hybrid.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To understand the performance of the Megatron GPT-3 pretraining, we can check the generated logs during the execution.\n",
    "\n",
    "Let's first look at the generated [logs](/dli/megatron/logs/log_2Nodes4GPUS_hybrid.txt) and check world size of our executed hybrid run. We should see this:\n",
    "\n",
    "```\n",
    "using world size: 4, data-parallel-size: 1, tensor-model-parallel size: 2, pipeline-model-parallel size: 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"using world size:\" /dli/megatron/logs/log_2Nodes4GPUS_hybrid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the training performance and GPU0 memory allocation and deallocation of our run. You should see a graph similar to the following: \n",
    "<img src=\"images/profiling_hybrid.png\" width=\"950\"/>\n",
    "\n",
    "What is the constant time step between the forward and backward pass? Discuss with the instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep iteration /dli/megatron/logs/log_2Nodes4GPUS_hybrid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints \n",
    "!rm -rf /dli/megatron/checkpoints/* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Before moving on, we need to make sure no jobs are still running or waiting in the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will see how to optimize the GPT pretraining using techniques such as Mixed Precision, Gradient Accumulation and Activation Checkpointing. Move on to [004_GPT_LM_pretrainings_optimizations.ipynb](04_GPT_LM_pretrainings_optimizations.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
