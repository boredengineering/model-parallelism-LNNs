{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview of the Class Environment\n",
    "\n",
    "Before we start looking at how to deploy large models, we should revisit the setup of the lab environment. In this section, we will experiment with tools for resource monitoring. The hardware used in this class may vary between sessions, so the number of GPUs, their memory capacity as well as their interconnect might vary from class to class. The results currently listed are based on 4 V100s with 16GB of memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals of this notebook are to: \n",
    "* Revisit the hardware configuration at our disposal. \n",
    "* Use key nvidia-smi commands to monitor NVIDIA GPUs. \n",
    "* Run test scripts to measure the peer-to-peer communication performance of the NVLINK bus which will be essential for model parallel communication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hardware Overview \n",
    "\n",
    "Let us have a look at the key components of the hardware system at our disposal. As discussed earlier, the configuration of the system does vary between deliveries so you might see different results than some of your classmates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CPU\n",
    "\n",
    "Let us start by inspecting the type of CPU used as well as the number of cores at our disposal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of CPU cores\n",
    "!grep 'cpu cores' /proc/cpuinfo | uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will host the NLP model alone and make a limited number of concurrent requests for that model. This will mainly use the GPU, but not much of the GPU. In the case of most production systems, one would deploy not just the model in isolation. An end-to-end production pipeline would include data pre and post processing steps. Production systems also experience much higher traffic, creating higher demand on the CPU, which needs to handle the processing of incoming requests (e.g. Triton Execution overheads). Therefore, maintaining the correct ratio between CPU and GPU resource is critical. Please reach out to your local NVIDIA team for a more detailed conversation about the design of inference systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GPU\n",
    "\n",
    "As before let us list the number and type of available GPUs. As the class environments will vary, there may be anywhere from four to eight Volta V100 GPUs with either 16G or 32G of onboard high bandwidth memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPUs\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interconnect Topology\n",
    "\n",
    "As discussed in lab 1, the GPUs we are using today are interconnected using [NVIDIA NVLink technology](https://www.nvidia.com/en-us/data-center/nvlink/). It allows workloads that have high bandwidth and low latency communication to overcome the limitations of PCIe technology. Inference of deep neural networks is in principle an \"embracingly parallel\" workload which is enhanced by the connectivity between the GPUs. Large models, that do not fit into a single GPU (such as recommender systems) create a high requirement for both required bandwidth and also latency. For such models, parallel deployments with NVLINK is a key technology enabling real time execution. Let us inspect the interconnect between the GPUs. Please use the `nvidia-smi topo --matrix` command below to check the topology of our NVLINK interconnect. Depending on the setup of the class we should see all 4 GPUs connected to each other (like in the example output listed below) or 8 GPUS interconnected (in this situation not all GPUs have direct NVLINK interconnect). \n",
    "\n",
    "```\n",
    "        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity\n",
    "GPU0     X      NV12    SYS     SYS     0-23            N/A\n",
    "GPU1    NV12     X      SYS     SYS     24-47           N/A\n",
    "GPU2    SYS     SYS      X      NV12    48-71           N/A\n",
    "GPU3    SYS     SYS     NV12     X      72-95           N/A\n",
    "\n",
    "Where X= Self and NV# = Connection traversing a bonded set of # NVLinks\n",
    "```\n",
    "\n",
    "On Ampere and Hopper based NVLINK enabled systems, one can find also NVSWITCH overcoming the above-mentioned limitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Interconnect Topology \n",
    "!nvidia-smi topo --matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check NVLink status and capabilities using `nvidia-smi nvlink --status` command. On a 4 GPU based system there should be an output listing NVLink capabilities of each GPU like the below:\n",
    "```\n",
    "GPU 0: Graphics Device\n",
    "\t Link 0: 25 GB/s\n",
    "\t Link 1: 25 GB/s\n",
    "\t Link 2: 25 GB/s\n",
    "\t Link 3: 25 GB/s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nvlink status\n",
    "!nvidia-smi nvlink --status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Connectivity\n",
    "\n",
    "Let's make an empirical measurement of the bandwidth and latency that we are achieving in our environment. NVIDIA provides an example application called  **p2pBandwidthLatencyTest** that demonstrates CUDA Peer-To-Peer (P2P) data transfers between pairs of GPUs by computing bandwidth and latency while enabling and disabling NVLink connections. This tool is part of the code samples for CUDA Developers [cuda-samples](https://github.com/NVIDIA/cuda-samples.git). It can be downloaded using the following command, but it was pre-downloaded for the purpose of this class: \n",
    "\n",
    "`git clone --depth 1 --branch v11.2 https://github.com/NVIDIA/cuda-samples.git` \n",
    "\n",
    "To test the bandwidth and latency, please execute the below commands. Please pay particular attention to comparison of results where \"P2P=Disabled\" vs \"P2P=Enabled\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 770 ./cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Now that we have reviewed the information about the lab environment, let's begin the model deployment. <br> \n",
    "\n",
    "Please proceed to the following notebook to start the next section of the lab: [Inference of the GPT-J 6b model with HuggingFace.](02_HFRunInferenceOfTheGPT-J.ipynb) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
