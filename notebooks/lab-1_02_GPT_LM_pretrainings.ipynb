{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Multi-GPU Training Strategies\n",
    "\n",
    "In this notebook, we will introduce the basic knowledge of distributed training strategies and experiments with [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), NVIDIA library for training transformer-based language models.\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Understand the mechanisms behind distributed training strategies\n",
    "* Run a simple distributed training using Megatron-LM scripts on 1 Node with data and tensor parallel distribution\n",
    "* Understand the basic outputs of Megatron-LM logs\n",
    "\n",
    "**[2.1 Introduction to Distributed Training Strategies](#1.1-The-hardware-overview)<br>**\n",
    "**[2.2 Single GPU Training Execution of Megatron-LM GPT Pretraining](#1.1-The-hardware-overview)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.1 Check The GPT pretraining Script](#1.2.1-Exercise:-Explore-the-Test-Set)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.1 Run the GPT pretraining Script](#1.2.1-Exercise:-Explore-the-Test-Set)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.3 Understanding Megatron-LM Execution Logs](#1.2.1-Exercise:-Explore-the-Test-Set)<br>\n",
    "**[2.3 Multi-GPU Training Execution of Megatron-LM GPT Pretraining](#1.2-The-SLURM-Cluster-overview)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.1 Exercise: Megatron-LM GPT pretraining execution on 2 GPU](#1.2.1-Exercise:-Explore-the-Test-Set)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.2 Understanding Multi-GPU Megatron-LM Execution Logs](#1.2.1-Exercise:-Explore-the-Test-Set)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.3 Model Distribution Considerations ](#1.2.1-Exercise:-Explore-the-Test-Set)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel Previous Running/Pending Jobs\n",
    "\n",
    "Before moving on, check that no jobs are still running or waiting on the SLURM queue. Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.1 Introduction to Distributed Training Strategies\n",
    "In distributed training mode, the goal is to split the training process across multiple machines. The most commonly used distribution strategies are **data** and **model** parallelism.\n",
    "\n",
    "\n",
    "## Data Distribution \n",
    "\n",
    "Neural Networks are usually trained using [Stochastic Gradient Descent algorithms](https://developer.nvidia.com/blog/a-data-scientists-guide-to-gradient-descent-and-backpropagation-algorithms/) consisting of splitting the dataset into batches processed sequentially. At the forward step, the feature maps are computed while at the backward pass, gradients are computed and averaged to determine the parameter updates. Finally, the next batch of data is processed once the model's parameters are updated.\n",
    "\n",
    "<img src=\"images/data_parallel.png\" width=\"600\"/>\n",
    "\n",
    "In data parallelism mode, the data is split across multiple machines, each will be processed by a copy of the same neural network hosted by each machine. Parameter updates are averaged from all machines and model updates are reflected on all copies.\n",
    "\n",
    "Since, with more processors (or alternatively higher data parallelism), the time duration of an epoch (i.e. entire dataset) reduces, this has the effect of speeding up training. Also, because the updated gradients have effectively seen a larger number of samples (due to increased global batch size), this has a positive effect on the time to convergence. The time taken per batch is still the same, with the added cost of gradient exchange communication. \n",
    "\n",
    "There are different strategies for implementing the exchange of gradients: \n",
    "- **Centralized** way, where a server machine is responsible for distributing data chunks, accumulating gradients, and updating model parameters. \n",
    "- **Decentralized** way, where each worker sends and collects gradients from others to aggregate and update the model’s parameters locally. In addition, the workers can deliver the computation at different speeds. So, model parameters can be updated in a synchronous way based on the worker's synchronization points. We can also use a relaxed strategy, allowing workers to operate with outdated parameters. This strategy may introduce inconsistency during the training.\n",
    "\n",
    "Several libraries offer data parallelism implementations such as [Horovod](https://github.com/horovod/horovod) which is compatible with several Deep Learning Frameworks such as TensorFlow, Keras, PyTorch, Apache MXNet. [NVIDIA APEX](https://nvidia.github.io/apex/) is a Pytorch extension library that offers utilities to streamline distributed training and Mixed Precision.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Model Distribution Strategies\n",
    "\n",
    "Model parallelism is the process of splitting a model’s parameters across multiple machines. This allows training bigger models that do not fit into 1 GPU, with the cost of additional communications due to feature maps exchange. \n",
    "\n",
    "We can distinguish 2 types of model distributions:\n",
    "\n",
    "### Pipeline Parallelism\n",
    "\n",
    "\n",
    "<img src=\"images/pipeline_parallel1.png\" width=\"600\"/>\n",
    "\n",
    "Pipeline Parallelism is the process of cutting sequentially a model into pieces and assigning each part to a specific worker. For instance layers 1,2 on device_1 and 3,4 on device_2, and so on. \n",
    "\n",
    "There are different pipelining strategies such as the Micro-batch pipeline parallelism [GPipe](https://arxiv.org/pdf/1811.06965.pdf), which is an optimized implementation of model pipelining to minimize the time machines wait for their peers to communicate their outputs. It consists in partitioning data chunks into micro batches enabling different machines to process different micro batches simultaneously.\n",
    "\n",
    "![title](images/pipeline_parallel.png)\n",
    "\n",
    "Instead of a single sequential set of layers per device, the [Interleaved pipeline parallelism](https://github.com/NVIDIA/Megatron-LM/) assigns multiple pipeline stages per device, each with less computation. \n",
    "For instance, layers 1,2 and 9,10 on device_1, layers 3,4 and 11,12 on device_2, and so on. \n",
    "\n",
    "\n",
    "### Tensor Parallelism\n",
    "\n",
    "<img src=\"images/tensor_parallel1.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Tensor Parallelism is the process of dividing matrix operations across workers. [Megatron-LM](https://github.com/NVIDIA/Megatron-LM/) is NVIDIA's open-source library for efficient training of transformer-based networks and multi-node pre-training of transformer-based models such as GPT, BERT, and T5 using mixed precision. In the Megatron's  transformer implementation, the Self-Attention and MLP operations are divided into parallel blocks with a total cost of 4 all-reduce operations per transformer cell (2 on the forward pass and 2 on the backward pass).\n",
    "\n",
    "<img src=\"images/tensor_parallel.png\" width=\"250\"/>\n",
    "\n",
    "Megatron-LM is built on top of PyTorch and it integrates data, pipeline and tensor parallelism for pre-training of GPT and BERT transformers architectures using mixed precision. In this lab, we will be using implemented distribution strategies provided by Megatron-LM library. In particular, we will focus in 1 Node execution on 1 or 2 devices and in the second case, we will use data, tensor and pipeline parallelism. Several examples can be found in the [Megatron-LM repository](https://github.com/NVIDIA/Megatron-LM/tree/main/examples). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.2 Single GPU Training Execution of Megatron-LM GPT Pretraining \n",
    "\n",
    "Let's first get familiarized with a simple Megatron-LM GPT execution script. \n",
    "\n",
    "For distributed training mode, scripts use the [PyTorch distributed launcher](https://pytorch.org/docs/stable/distributed.html). The PyTorch distributed module is used with the Python flag `-m torch.distributed.launch`. \n",
    "\n",
    "The resources are configured with the arguments `--nnodes` and `--nproc_per_node` specify respectively the number of nodes and GPUs per node to use.\n",
    "\n",
    "With Megatron-LM library, there are 2 types of distributed data parallel implementations: \n",
    "- `local` performing gradient all-reduce at the end of the back propagation step \n",
    "- `torch` distributed data parallel wrapper that overlaps the gradient reduction computation with the back propagation computation (more efficient at larger model sizes).\n",
    "\n",
    "In this section, we will run a simple Megatron-LM GPT Pretraining Execution on 1 GPU by running the Megatron-LM pretrain_gpt.py script with the corresponding arguments.\n",
    "\n",
    "<img src=\"images/Megatron_run.PNG\" width=\"600\"/>\n",
    "We can specify the distributed data parallel implementation using the argument `--DDP-impl`.\n",
    "\n",
    "Distributed strategies are configured with the arguments `--tensor-model-parallel-size` and `--pipeline-model-parallel-size`\n",
    "Learn more about the distributed strategies arguments in the [Megatron-LM documentation](https://github.com/NVIDIA/Megatron-LM#distributed-pretraining)\n",
    "\n",
    "We have prepared the script [pretrain_gpt_1GPU.sh](/dli/code/pretrain_gpt_1GPU.sh) that will run GPT pretraining on only 1 GPU (with no distribution strategy applied).\n",
    "\n",
    "This script assumes that the compute resources are already allocated. Thus, for the execution, we will need to first allocate the required GPU by connecting to a worker node in an interactive session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Check The GPT pretraining Script\n",
    "\n",
    "Let's have a look at the script before allocating the resources and executing it. \n",
    "\n",
    "Notice the model architecture and training arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the Megaton-LM GPT pretraining execution on 1 GPU script\n",
    "!cat /dli/code/pretrain_gpt_1GPU.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2.2 Run the GPT pretraining Script\n",
    "\n",
    "Now, let's run the pretrain_gpt_1GPU.sh script in an interactive session. To do so, follow the 3 steps:\n",
    "1. Launch a terminal session\n",
    "2. Run an interactive session by executing `srun -N 1 --pty /bin/bash`\n",
    "3. Run the Megatron GPT-3 pretraining on 1 GPU by executing `bash ./code/pretrain_gpt_1GPU.sh`\n",
    "\n",
    "\n",
    "<img src=\"images/interactive_launch0.png\" width=\"1050\"/>\n",
    "\n",
    "Run the following cell to generate a link to open a terminal session and the instructions to run interactive session. Then, submit a GPT pretraining job on 1 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
    "   Step 3: Run the megatron gpt3 pretraining on 1 GPU: <font color=\"green\">bash ./code/pretrain_gpt_1GPU.sh</font>\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the GPT pretraining on 1 GPU is running. We can check the SLURM queue by running this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the GPUs using the `nvidia-smi` command. We should see only GPU 0 utilized as shown in the figure bellow. Please notice that the first time Megatron-LM is running, the code will need about 6 minutes to be compiled. Until there, you will not be able to see any GPU activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1N_1gpu_utilization.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU utilization on the master node after Megatron-LM is compiled\n",
    "!sleep 6m\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3  Understanding Megatron-LM Execution Logs \n",
    "\n",
    "As specified in the pretrain_gpt_1GPU.sh script, the world size Megatron-LM is executed should be as follows\n",
    "```\n",
    "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/interactive_launch1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of the GPT pretraining, we can check the generated [log file](./megatron/logs/log_1GPU.txt) during the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Megatron GPT3 pretraining logs.\n",
    "!grep iteration /dli/megatron/logs/log_1GPU.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the extract, the outputs should be similar to:\n",
    "\n",
    "```\n",
    " iteration      100/     100 | consumed samples:          200 | elapsed time per iteration (ms): 271.6 | learning rate: 5.822E-05 | global batch size:     2 | lm loss: 7.587920E+00 | loss scale: 1.0 | grad norm: 1.468 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
    "```   \n",
    "\n",
    "In this example, notice the training speed of 271.6ms to process 2 samples (global batch size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Great! Before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution and cancel the remaining interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints \n",
    "!rm -rf /dli/megatron/checkpoints/*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 2.3 Multi-GPU Training Execution of Megatron-LM GPT Pretraining\n",
    "\n",
    "Let's now execute the same previous training job while using the 2 GPUs available in the interactive session. \n",
    "\n",
    "Using `torch.distributed.launch` to launch jobs on 2 GPUs, we need to set the number of processes per node to `--nproc_per_node 2`. \n",
    "\n",
    "The first distribution strategy we will experiment with is the data parallel distribution strategy, which is executed by default with Megatron-LM when several resources are available.\n",
    "             \n",
    "In the previous execution on one single GPU, the batch size processed by the GPU was 2 (set by `--micro-batch-size`) which also corresponds to the global batch size (set by `--global-batch-size`). \n",
    "\n",
    "\n",
    "## 2.3.1 Exercise: Megatron-LM GPT pretraining execution on 2 GPUs\n",
    "Let's configure the new Megatron-LM GPT pretraining execution on 2 GPUs using data parallel distribution by modifying the \"FIXME\" in the following cell. \n",
    "\n",
    "To use 2 GPUs, we can keep the micro batch size per GPUs to 2 and thus double the global batch size to 4. If you get stuck, feel free to look at the [solution](solutions/ex2.3.ipynb).\n",
    "\n",
    "Please notice that we will change the logfile name for each run (*log_2GPU.txt* in the following example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2GPU.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=1\n",
    "GPUS_PER_NODE=#FIXEME         # <--- CHANGE HERE\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=2\n",
    "GLOBAL_BATCH_SIZE=#FIXEME    # <--- CHANGE HERE\n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "VOCAB_SIZE=50257\n",
    "\n",
    "# Data Paths\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=/dli/data/GPT-2_assets/my-gpt2_text_document\n",
    "\n",
    "DATA_OUTPUT_PATH=/dli/megatron/checkpoints/test\n",
    "CHECKPOINT_PATH=/dli/megatron/checkpoints\n",
    "TENSORBOARD_PATH=/dli/megatron/tensorboard\n",
    "LOGS_PATH=/dli/megatron/logs\n",
    "NAME=\"log_2GPU\"        \n",
    "\n",
    "# SLURM args\n",
    "MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
    "MASTER_PORT=6000\n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            --optimizer adam \\\n",
    "            --adam-beta1 0.9 \\\n",
    "            --adam-beta2 0.95 \\\n",
    "            --adam-eps 1e-8 \\\n",
    "            --lr 6e-5 \\\n",
    "            --min-lr 6e-6 \\\n",
    "            --lr-decay-style cosine \\\n",
    "            --lr-decay-iters 800 \\\n",
    "            --lr-warmup-fraction .01 \\\n",
    "            --clip-grad 1.0 \\\n",
    "            --weight-decay 1e-1 \\\n",
    "            --exit-duration-in-mins 1190 \\\n",
    "            \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            --num-layers $NLAYERS \\\n",
    "            --hidden-size $NHIDDEN \\\n",
    "            --num-attention-heads $NHEADS \\\n",
    "            --seq-length $SEQ_LEN \\\n",
    "            --max-position-embeddings $SEQ_LEN \\\n",
    "            --micro-batch-size $MICRO_BATCH_SIZE \\\n",
    "            --global-batch-size $GLOBAL_BATCH_SIZE \\\n",
    "            --train-iters 100 \\\n",
    "            --vocab-file $VOCAB_FILE \\\n",
    "            --merge-file $MERGE_FILE \\\n",
    "            --init-method-std 0.006 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "            $EXIT_OPTS \\\n",
    "            \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            --log-interval 10 \\\n",
    "            --save-interval 300 \\\n",
    "            --eval-interval 1000 \\\n",
    "            --eval-iters 10 \\\n",
    "            --tensorboard-dir $TENSORBOARD_PATH \\\n",
    "            --tensorboard-queue-size 1 \\\n",
    "            --log-timers-to-tensorboard \\\n",
    "            --log-batch-size-to-tensorboard \\\n",
    "            --log-validation-ppl-to-tensorboard \\\n",
    "            \"\n",
    "export LAUNCHER=\"python -u -m torch.distributed.launch \\\n",
    "            --nproc_per_node $GPUS_PER_NODE \\\n",
    "            --nnodes $NNODES \\\n",
    "            --master_addr $MASTER_ADDR \\\n",
    "            --master_port $MASTER_PORT \\\n",
    "            \"\n",
    "\n",
    "export CMD=\" \\\n",
    "            /dli/megatron/Megatron-LM/pretrain_gpt.py \\\n",
    "            --tensor-model-parallel-size $TP_SIZE \\\n",
    "            --pipeline-model-parallel-size $PP_SIZE \\\n",
    "            $GPT_ARGS \\\n",
    "            $OUTPUT_ARGS \\\n",
    "            --save $CHECKPOINT_PATH \\\n",
    "            --data-path $DATA_PATH \\\n",
    "            --data-impl mmap \\\n",
    "            --split 949,50,1 \\\n",
    "            --distributed-backend nccl \\\n",
    "            \"\n",
    "\n",
    "bash -c '$LAUNCHER  $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's run this script in an interactive session. To do so, follow the 3 steps:\n",
    "1. Launch a terminal session\n",
    "2. Run an interactive session by executing `srun -N 1 --pty /bin/bash`\n",
    "3. Run the megatron gpt3 pretraining on 1 GPU by executing `bash ./code/pretrain_gpt_2GPU.sh`\n",
    "\n",
    "Run the following cell to get the link to open a terminal session and the instructions to run an interactive session. Then, submit a pretraining job on 2 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
    "   Step 3: Run the megatron gpt3 pretraining on 1 GPU: <font color=\"green\">bash ./code/pretrain_gpt_2GPU.sh</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the GPT pretraining on 1 Node and 2 GPUs is running. We can check the SLURM queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also Check the GPUs using the `nvidia-smi` command. We should see GPU 0 and 1 utilized as shown in the figure bellow.\n",
    "\n",
    "<img src=\"images/1N_2gpus_utilization.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU utilization on the master node\n",
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Understanding Multi-GPU Megatron-LM Execution Logs\n",
    "\n",
    "Let's have a look at the execution logs:\n",
    "\n",
    "<img src=\"images/interactive_launch2.png\" width=\"900\"/>\n",
    "\n",
    "The world size Megatron-LM will be executing should be as follows: \n",
    "```\n",
    "world size: 2, data-parallel-size: 2, tensor-model-parallel size: 1, pipeline-model-parallel size: 1\n",
    "```\n",
    "As we have 2 GPUs available, by default, the distributed strategy executed is the data parallel strategy, meaning that the model is copied on both GPUs and will process different data batches. \n",
    "\n",
    "To understand the performance of the GPT pretraining on 2 GPUS, we can check the generated [log file](/dli/megatron/logs/log_2GPU.txt) during the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep iteration /dli/megatron/logs/log_2GPU.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the extract logs, notice the training performance while using 2 GPUs compared to 1 GPU.\n",
    "\n",
    "` iteration      100/     100 | consumed samples:          400 | elapsed time per iteration (ms): 363.6 | learning rate: 5.822E-05 | global batch size:     4 | lm loss: 7.500983E+00 | loss scale: 1.0 | grad norm: 1.360 | number of skipped iterations:   0 | number of nan iterations:   0 |`\n",
    " \n",
    " \n",
    " \n",
    "Notice the number of samples consumed, and the corresponding training time. Notice also that this is an almost linear increase which is a desirable property in multi-GPU systems.  \n",
    "\n",
    "Discuss the performance with the instructor. The major change here is larger number of samples processed in the same time duration, therefore helping the model learn richer data representations, speeding up the training.\n",
    "\n",
    "Great, before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution and cancel the remaining interactive session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints\n",
    "!rm -rf /dli/megatron/checkpoints/*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3 Model Distribution Considerations \n",
    "\n",
    "To execute the previous Multi-GPU script in Tensor or Pipeline parallel mode, we can configure the distribution using the argument `--tensor-model-parallel-size` or `--pipeline-model-parallel-size`. \n",
    "\n",
    "The world size of Megatron-LM training corresponding to the number of GPUs will remain the same while the data-parallel, tensor-model-parallel and pipeline-model-parallel should be adjusted according to your configuration: \n",
    "```\n",
    "world size: 2, data-parallel-size: 1, tensor-model-parallel size: 2, pipeline-model-parallel size: 1\n",
    "or\n",
    "world size: 2, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 2\n",
    "\n",
    "```\n",
    "The world size is the product of data-parallel-size, tensor-model-parallel and pipeline-model-parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Great job with pretraining GPT-3 on a GPU cluster.<br>\n",
    "\n",
    "Before moving on, we need to make sure that no jobs are still running or waiting on the SLURM queue. \n",
    "Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be running  GPT language model training on multi-nodes distribution configurations. Move on to [03_GPT_LM_pretrainings_multinodes.ipynb](03_GPT_LM_pretrainings_multinodes.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
