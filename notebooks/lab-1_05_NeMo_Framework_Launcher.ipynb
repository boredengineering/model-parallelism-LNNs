{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Introduction to NeMo Framework Launcher\n",
    "\n",
    "In this notebook, we will learn how to use [NeMo Framework Launcher](https://github.com/NVIDIA/NeMo-Megatron-Launcher) to conveniently generate configuration files and SLURM scripts for NeMo jobs.\n",
    "\n",
    "## The goals\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Learn how to use NeMo Framework Launcher to speed up launching end-to-end NeMo Framework training jobs\n",
    "* Understand how to examine intermediate configuration files and scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# NeMo Framework Launcher\n",
    "\n",
    "The NeMo Framework Launcher is designed to be a simple and easy to use tool for launching NeMo Framework training jobs on CSPs or on-prem clusters.\n",
    "\n",
    "It takes care of generating and launching job submission scripts, as well as storing job results. It also comes packaged with tested configuration files, which can be easily modified by user.\n",
    "\n",
    "<img src=\"images/nemo_launcher.png\" width=\"800\"/>\n",
    "\n",
    "The most convenient way to use NeMo Framework Launcher is with the [NeMo FW Container](https://registry.ngc.nvidia.com/orgs/ea-bignlp/teams/ga-participants/containers/nemofw-training), the access for which can be applied for [here](https://developer.nvidia.com/nemo-framework).\n",
    "\n",
    "Let's get started with NeMo Framework Launcher. We will begin with examining an [example configuration file for GPT3 126m](./code/NeMo-Megatron-Launcher/launcher_scripts/conf/training/gpt3/126m.yaml):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /dli/code/NeMo-Megatron-Launcher/launcher_scripts/conf/training/gpt3/126m.yaml | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the similarity with some arguments we modified in the previous notebooks, in particular, the `trainer` section. Just as before, we will need to overwrite some of them directly from the command line using [Hydra](https://hydra.cc/docs/intro/). Note that we can omit the section with SLRUM configuration.\n",
    "\n",
    "As NeMo Framework Launcher supports running different types of training/finetuning/data preparation jobs, we will need to specify the desired model development stage in `stages=[]` and prepend job arguments with corresponding prefix. For us, that would be `training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_126m_nemo_fw.sh   \n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=4    \n",
    "GLOBAL_BATCH_SIZE=64\n",
    "\n",
    "# Data Paths\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=[1.0,/dli/data/GPT-2_assets/my-gpt2_text_document]\n",
    "\n",
    "OUTPUT_PATH=/dli/nemo\n",
    "LOGS_PATH=/dli/nemo/logs\n",
    "NAME=\"GPT_126m_NeMo_FW\"      \n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            training.model.optim.name=fused_adam \\\n",
    "            training.model.optim.betas=[0.9,0.95] \\\n",
    "            training.model.optim.lr=6e-5 \\\n",
    "            training.model.optim.sched.min_lr=6e-6 \\\n",
    "            training.model.optim.sched.name=CosineAnnealing \\\n",
    "            +training.model.optim.sched.max_steps=800 \\\n",
    "            training.model.optim.sched.warmup_steps=80 \\\n",
    "            training.model.optim.weight_decay=1e-1 \\\n",
    "        \"\n",
    "\n",
    "# NeMo Framework Launcher arguments\n",
    "LAUNCHER_ARGS=\" \\\n",
    "            cluster_type=bcm \\\n",
    "            stages=[training] \\\n",
    "            training=gpt3/126m \\\n",
    "            training_config=gpt3/126m \\\n",
    "            launcher_scripts_path=/dli/code/NeMo-Megatron-Launcher/launcher_scripts \\\n",
    "            \"\n",
    "\n",
    "# Search path for NeMo example configs\n",
    "HYDRA_ARGS=\" \\\n",
    "            training.hydra.searchpath=[file:///dli/code/NeMo/examples/nlp/language_modeling/conf]\n",
    "        \"\n",
    "\n",
    "# Trainer arguments\n",
    "TRAINER_ARGS=\" \\\n",
    "            training.trainer.devices=$GPUS_PER_NODE \\\n",
    "            training.trainer.num_nodes=$NNODES \\\n",
    "            training.trainer.max_steps=1000 \\\n",
    "            +training.trainer.enable_model_summary=true \\\n",
    "            training.trainer.log_every_n_steps=10 \\\n",
    "            training.trainer.val_check_interval=20 \\\n",
    "            training.trainer.limit_val_batches=10 \\\n",
    "            +training.trainer.use_profiler=true \\\n",
    "        \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            training.model.micro_batch_size=$MICRO_BATCH_SIZE \\\n",
    "            training.model.global_batch_size=$GLOBAL_BATCH_SIZE \\\n",
    "            training.model.tokenizer.vocab_file=$VOCAB_FILE \\\n",
    "            training.model.tokenizer.merge_file=$MERGE_FILE \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "        \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            training.run.results_dir=$OUTPUT_PATH/$NAME \\\n",
    "            training.exp_manager.explicit_log_dir=$OUTPUT_PATH/$NAME \\\n",
    "            training.exp_manager.resume_if_exists=false \\\n",
    "            training.exp_manager.name=$NAME \\\n",
    "        \"\n",
    "\n",
    "PARALLEL_ARGS=\" \\\n",
    "            training.model.tensor_model_parallel_size=$TP_SIZE \\\n",
    "            training.model.pipeline_model_parallel_size=$PP_SIZE \\\n",
    "        \"\n",
    "\n",
    "CMD=\" \\\n",
    "            python /dli/code/NeMo-Megatron-Launcher/launcher_scripts/main.py \\\n",
    "            $LAUNCHER_ARGS \\\n",
    "            $HYDRA_ARGS \\\n",
    "            $TRAINER_ARGS \\\n",
    "            $GPT_ARGS \\\n",
    "            $OUTPUT_ARGS \\\n",
    "            $PARALLEL_ARGS \\\n",
    "            training.model.data.data_prefix=$DATA_PATH \\\n",
    "            training.model.data.data_impl=mmap \\\n",
    "            training.model.data.splits_string=\\\"949,50,1\\\" \\\n",
    "        \"\n",
    "\n",
    "$CMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the NeMo Framework Launcher script. It will generate the submission script and will run it. \n",
    "\n",
    "**Special Warning:** running generated script will fail, as it's expected to be run in a different environment compared to the one used in the course. We are just interested in examining the resulting script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /dli/code/pretrain_gpt_126m_nemo_fw.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the generated script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /dli/nemo/GPT_126m_NeMo_FW/nemo-megatron-gpt3_126m_submission.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the script already contains configured parameters for SLURM (starting from `#SBATCH`), updated environment variables, and command to run the job (starting from `srun`). NeMo Framework Launcher also generated .yaml config file for the job in `/dli/nemo/GPT_126m_NeMo_FW` with a name of `gpt3_126m_hydra.yaml`.\n",
    "\n",
    "Note the generated run command, starting from `srun`. It has arguments `container-image` and `container-mounts`, as it's expected that every node in the cluster will start the necessary container (in this case, `ga-participants/nemofw-training`), and run the job inside it. Running containerized workloads requires installing [enroot](https://github.com/NVIDIA/enroot) and [Pyxis](https://github.com/NVIDIA/pyxis) on your SLURM cluster. In this course, we are not going to run the generated script due to differences between the course environment and NeMo Framework Launcher containers, as well as to avoid running Docker containers inside Docker containers.\n",
    "\n",
    "Finally, let's examine the [generated config file](./nemo/GPT_126m_NeMo_FW/gpt3_126m_hydra.yaml):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /dli/nemo/GPT_126m_NeMo_FW/gpt3_126m_hydra.yaml | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with some slight differences it has the same structure and is intended to run in a similar way to how we ran NeMo Framework jobs before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lab, we will experiment with other techniques used for training large-scale neural networks and demonstrate their usage for Computer Vision. Move on to [06_Multi-Nodes_Distributed_Training_for_Computer_Vision.ipynb](06_Multi-Nodes_Distributed_Training_for_Computer_Vision.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
