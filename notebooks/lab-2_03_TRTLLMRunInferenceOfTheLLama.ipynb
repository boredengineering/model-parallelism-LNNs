{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimizing inference with NVIDIA TensorRT-LLM library \n",
    "\n",
    "In this lab, we are going to look at the NVIDIA TensorRT-LLM library and how it optimizes execution of large language models. We will use it to deploy Llama 13B initially using just a single GPU but afterwards taking advantage of its Tensor and Pipeline parallelism capabilities on multiple GPUs.  \n",
    "\n",
    "We will conclude this notebook by comparing the latency between our baseline implementation using the Transformers library and the TensorRT-LLM Tensor and Pipeline parallel deployments. In the next notebook, we will look at how to serve our TensorRT-LLM optimized model to customers/users using Triton Inference Server. \n",
    "\n",
    "To summarize, in this notebook we will: \n",
    "* Review the features of NVIDIA TensorRT-LLM library. \n",
    "* Learn how to build the development environment including building TensorRT-LLM library. \n",
    "* Learn how to prepare a checkpoint of LLama2 (or other Transformers based model) for inference with TensorRT LLM. \n",
    "* Run inference of the model on a single GPU. \n",
    "* Extend the execution to multiple GPUs using Tensor Parallelism. \n",
    "* Profile the single and multi-GPU pipelines to capture information about throughput and latency. \n",
    "\n",
    "**[3.1 NVIDIA TensorRT-LLM](#3.1)<br>** \n",
    "**[3.2 Overall Inference Pipeline with NVIDIA TensorRT-LLM](#3.2)<br>** \n",
    "**[3.3 Download and Install NVIDIA TensorRT-LLM library](#3.3)<br>** \n",
    "**[3.4 Download LLama2 weights](#3.4)<br>** \n",
    "**[3.5 Compile TensorRT-LLM engines](#3.5)<br>** \n",
    "**[3.6 Run TensorRT-LLM engines](#3.6)<br>** \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.6.1 Inference on 1 GPU ](#3.6.1)<br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.6.2 Inference on 2 GPUs ](#3.6.2)<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 NVIDIA TensorRT-LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "\n",
    "In 2020, OpenAI demonstrated that using a large language model trained in a self-supervised way on large volume of training data can significantly improve the capacity of GPT model ([refer to the paper for more details](https://arxiv.org/abs/2005.14165)). The largest GPT-3 variant, has 175 billion parameters, which consumes about 350 GBs, even when represented in half-precision. Therefore putting such a model on a single GPU is impossible, making multi-GPU or even multi-node deployment a necessity. To solve the challenges of latency and memory footprint, the FasterTransformer library provides high efficiency kernels, optimized for memory usage, and support for model parallelism.</br>\n",
    "[NVIDIA’s TensorRT-LLM (FT)](https://github.com/NVIDIA/TensorRT-LLM) is an open-source library for optimal performance on the latest Large Language Models for inference on NVIDIA GPUs. It consists of the TensorRT deep learning compiler and includes optimized kernels, pre– and post-processing and multi-GPU/multi-node communication primitives steps – largely inspired from the former Faster Transformer library for groundbreaking performance on NVIDIA GPUs.\n",
    "It enables you to experiment with new LLMs, with peak performance and quick customization capabilities, without requiring a deep knowledge of C++ or NVIDIA CUDA, as it is offering a convenient Python API.</br>\n",
    "TensorRT-LLM comes with several popular models pre-defined. They can easily be modified and extended to fit custom needs. See for a list of supported [models](https://github.com/NVIDIA/TensorRT-LLM/tree/main/tensorrt_llm/models).\n",
    "\n",
    "It also comes with a wide range of other features including:</br> \n",
    "* Number of attention layers and caching methods:\n",
    "    * Multi-head Attention(MHA)\n",
    "    * Multi-query Attention (MQA)\n",
    "    * Group-query Attention(GQA)\n",
    "    * Paged KV Cache for the Attention\n",
    "* Support for a range of data types and quantization methods:\n",
    "    * INT4/INT8 Weight-Only Quantization (W4A16 & W8A16)\n",
    "    * FP8\n",
    "    * SmoothQuant\n",
    "    * GPTQ, AWQ\n",
    "* Advanced feature: \n",
    "    * In-flight Batching\n",
    "    * Tensor Parallelism\n",
    "    * Pipeline Parallelism\n",
    "    * Greedy-search\n",
    "    * Beam-search\n",
    "    * RoPE\n",
    "\n",
    "This section of the notebook discusses how TensorRT-LLM can be used for optimization of the LLama2 model. It explains the optimization workflow for both single and multi GPU deployments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor and Pipeline Parallelism \n",
    "\n",
    "Under the hood, TensorRT-LLM relies on MPI and NVIDIA NCCL to enable inter/intra node communication. Using this software stack, anyone can run huge Transformers in Tensor-Parallelism mode on multiple GPUs to reduce computational latency. At the same time, tensor parallelism and pipeline parallelism can be combined to execute large models with billions and trillions of parameters (which amount to terabytes of weights) in Multi-GPU and Multi-Node environments. \n",
    "\n",
    "We have discussed the techniques below in the lecture but let us revisit them before diving into the implementation detail: \n",
    "- Data Parallelism (DP) - is a technique used during the training process. Every GPU receives the same copy of the model but different data to process. The GPUs execute the forward pass in parallel and exchange the gradients during the backward pass, allowing all the devices to make a synchronized weights update based on the average of the accumulated gradients. \n",
    "- Tensor Parallelism (TP) - is a technique used both during training and inference. Instead of splitting the data across multiple GPUs, selected layers of the model are distributed. If using Tensor Parallelism across 8 GPUs each layer affected/its tensor is split into 8 segments, each processed on a separate GPU in parallel. The results are gathered at the end of the step. \n",
    "- Pipeline Parallelism (PP) - similarly, this is a technique used both in training and inference. Here, individual layers are not being split into pieces, instead they are sequentially distributed across multiple GPUs. E.g. if training a 10 layer deep neural network across 2 GPUs, the first five layers would be deployed on the first GPU and the rest on the second GPU. Each GPU is processing data sequentially and the second GPU needs to wait for results from the first GPU. \n",
    "\n",
    "The diagram below demonstrates the difference between Tensor and Pipeline parallelism. \n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"./images/image3.png\" style=\"width: 1000px;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations in TensorRT-LLM library \n",
    "\n",
    "  \n",
    "\n",
    "TensorRT-LLM allows us to speed up the inference pipeline achieving lower latency and higher throughput compared to the common deep learning frameworks. Below are the key optimization techniques that allow TensorRT-LLM to achieve its performance: \n",
    "1. <b>Layer Fusion</b></br> \n",
    "During the model pre-processing stage, certain layers can be combined to form individual execution kernels. This allows for considerable reduction in GPU memory bandwidth increasing mathematical density of our model, thus accelerating computation at the inference stage. For example, all operations in the multi-head attention block can be combined into a single kernel. \n",
    "2. <b>Autoregressive models: Keys/Values caching. </b></br> \n",
    "In the generation phase, a common optimization is to provide the MultiHeadAttention kernel with a cache containing the values of the past K and V elements that have already been computed. That cache is known as the KV cache. The diagram below illustrates the process. TensorRT-LLM uses that technique to accelerate its generation phase. In TensorRT-LLM, there is one KV cache per Transformer layer, which means that there are as many KV caches as layers in a model. The current version of TensorRT-LLM supports two different types of KV caches: contiguous and paged KV caches.<br/> \n",
    "<div style=\"text-align:center\"> \n",
    "<img src=\"./images/KV_caching v2.PNG\" style=\"width: 50%;position:relative;\"><br/> \n",
    "<em>Keys/Values caching</em> \n",
    "</div> \n",
    "<br/><br/> \n",
    "3. <b>Usage of MPI and NCCL to enable inter/intra node communication and support model parallelism. </b></br> \n",
    "TensorRT-LLM adds the support for systems with multiple GPUs and nodes. It is enabled using TensorRT plugins that wrap communication primitives from the NCCL library as well as a custom plugin that optimize the All-Reduce primitive in the presence of All-to-all connections between GPUs (through NVSwitch in DGX systems).</br> \n",
    "Tensor Parallelism usually leads to more balanced executions but requires more memory bandwidth between the GPUs. Pipeline Parallelism reduces the need for high-bandwidth communication but may incur load-balancing issues and may be less efficient in terms of GPU utilization.</br>\n",
    "4. <b>Reduced precision inference</b></br> \n",
    "TensorRT-LLM has kernels that support inference using low-precision input data in fp32, fp16, bf6, fp8, int8 and int4. All these regimes allow acceleration due to the reduction in data transfer and required memory. Int8 and fp16 computations can be hardware accelerated using TensorCores (available on all GPU architectures starting from Volta) and fp8 using Transformer Engines (starting from Hopper)</br>\n",
    "5. <b>Other optimizations include:</b></br> \n",
    "TensorRT-LLM supports in-flight batching of requests (also known as continuous batching or iteration-level batching) for higher serving throughput. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Overall Inference Pipeline with NVIDIA TensorRT-LLM\n",
    "The diagram listed below lists all the steps involved in using the TensorRT-LLM library to deploy large models to production. In the next section, we will go through them one at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/TRTLLM_pipeline.png\" style=\"width: 30%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.3 Download and install NVIDIA TensorRT-LLM library\n",
    "Starting with Triton 23.10 release, Triton includes a container with the TensorRT-LLM Backend and the Python Backend. This container should have everything needed to run a TensorRT-LLM model. You can find this container [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver).\n",
    "\n",
    "In this lab, we already git cloned the backend repository for you by using the commands: </br>\n",
    "```\n",
    "git clone -b release/0.6.1 https://github.com/triton-inference-server/TensorRT-LLM_backend.git\n",
    "cd tensorrtllm_backend \n",
    "```\n",
    "We then fetched the TensorRT-LLM library as a submodule:</br>\n",
    "```\n",
    "git submodule update --init --recursive\n",
    "git lfs install\n",
    "git lfs pull\n",
    "```\n",
    "And install properly the TensorRT-LLM library in the Triton container: </br>\n",
    "```\n",
    "pip install git+https://github.com/NVIDIA/TensorRT-LLM.git\n",
    "mkdir /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/\n",
    "cp /opt/tritonserver/backends/tensorrtllm/* /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Download LLama Weights\n",
    "We already downloaded the model weights from the [Meta website](https://llama.meta.com/). Please view the [license agreement](https://ai.meta.com/llama/license/). If you would like to use a Llama model outside of this course, please [register with Meta](https://llama.meta.com/llama-downloads).\n",
    "\n",
    "Once downloaded, we converted the weights into Hugging Face format using the `src/transformers/models/llama/convert_llama_weights_to_hf.py` python script from Transformers Library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>/dli/task/weights</b> folder's content should look similar to the following:\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/llama_weights_folder_new.png\" style=\"width: 50%\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Build TensorRT-LLM engines\n",
    "### Build on 1 GPU\n",
    "\n",
    "In this section, we are going to build TensorRT-LLM engines from the Hugging Face Llama weights, first on 1 GPU, and then on 4 GPUs using model parallelism.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /dli/task/tensorrtllm_backend/tensorrt_llm/examples/llama\n",
    "%pip install -r requirements.txt\n",
    "%pip install --upgrade protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the output directory where to store the compiled engine: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /dli/task\n",
    "trt_engine_1gpu=\"/dli/task/trt-engines/llama_13b/fp16/1-gpu\"\n",
    "!mkdir -p $trt_engine_1gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Build command takes some parameters, each of them impacting performance of the engine at different level (Memory used for KV Caching, Batching policy, Quantization, ...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_weights_dir = \"/dli/task/weights\"\n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/build.py  \\\n",
    "                --model_dir $hf_weights_dir \\\n",
    "                --dtype float16 \\\n",
    "                --use_gpt_attention_plugin float16  \\\n",
    "                --use_inflight_batching \\\n",
    "                --paged_kv_cache \\\n",
    "                --remove_input_padding \\\n",
    "                --use_gemm_plugin float16  \\\n",
    "                --output_dir $trt_engine_1gpu  \\\n",
    "                --max_input_len 2048 --max_output_len 512 \\\n",
    "                --use_rmsnorm_plugin float16  \\\n",
    "                --enable_context_fmha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your output ! You should have .engine in the folder now\n",
    "!ls $trt_engine_1gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build on 4 GPUs\n",
    "Large Language Models can be huge and the GPU RAM can be a limitation.\n",
    "Pipeline and Tensor Parallelism (PP and TP) are efficient ways to workaround the memory limitation on a single GPU as they split the model into parts at training and inference time and distribute them among multiple GPUs.</br>\n",
    "Let's see it in action on Llama-13B.\n",
    "The world size will represent the number of parts you have.\n",
    "For example, using PP=2 and TP=2, the world size is equal to 4.\n",
    "\n",
    "Prepare the output directory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_engine_4gpus= \"/dli/task/trt-engines/llama_13b/fp16/4-gpus\"\n",
    "!mkdir -p $trt_engine_4gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your engine using Tp_size and PP_size flags\n",
    "World_size must be equal to  Tp_size * PP_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\\n",
    "    --model_dir $hf_weights_dir \\\n",
    "    --dtype float16 \\\n",
    "    --use_gpt_attention_plugin float16 \\\n",
    "    --use_gemm_plugin float16 \\\n",
    "    --use_rmsnorm_plugin float16 \\\n",
    "    --use_inflight_batching \\\n",
    "    --remove_input_padding \\\n",
    "    --enable_context_fmha \\\n",
    "    --paged_kv_cache \\\n",
    "    --max_input_len 2048 --max_output_len 512 \\\n",
    "    --output_dir $trt_engine_4gpus \\\n",
    "    --world_size 4 \\\n",
    "    --tp_size 2 \\\n",
    "    --pp_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your output ! You should have 4 engines in the folder, one for each rank\n",
    "!ls $trt_engine_4gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Run the TensorRT-LLM engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on 1 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your engine are now ready to run using TensorRT-LLM library! \n",
    "The tokenizer files are stored next to the weights in the Llama folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "    --engine_dir=$trt_engine_1gpu \\\n",
    "    --max_output_len 128 \\\n",
    "    --tokenizer_dir $hf_weights_dir \\\n",
    "    --input_text \"How do I count in French ? 1 un\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on 4 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mpirun command to launch the Run command on TensorRT-LLM for multi-GPUs execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -n 4 --allow-run-as-root python tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "    --engine_dir=$trt_engine_4gpus\\\n",
    "    --max_output_len 128 \\\n",
    "    --tokenizer_dir $hf_weights_dir \\\n",
    "    --input_text \"How do I count in French? 1 un \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Exercice - Build and Run on 2 GPus\n",
    "Let's practice yourself, and try to create a TensorRT-LLM engine on 2 GPU using only Tensor Parallelism.</br>\n",
    "1) Prepare your output directory </br>\n",
    "2) Build the engine </br>\n",
    "3) Run and test your engine </br>\n",
    "\n",
    "Fill the <<<< FIXME >>>> in the cells below. If you are stuck, check the solutions clicking on the ... just under each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepare your output directory\n",
    "trt_engine_2gpus= <<<<FIXME>>>>\n",
    "!mkdir -p $trt_engine_2gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SOLUTION \n",
    "# 1) Prepare your output directory\n",
    "trt_engine_2gpus= \"/dli/task/trt-engines/llama_13b/fp16/2-gpus\"\n",
    "!mkdir -p $trt_engine_2gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build the engine \n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\\n",
    "    --model_dir <<<<FIXME>>>> \\\n",
    "    --dtype float16 \\\n",
    "    --use_gpt_attention_plugin float16 \\\n",
    "    --use_gemm_plugin float16 \\\n",
    "    --use_rmsnorm_plugin float16 \\\n",
    "    --use_inflight_batching \\\n",
    "    --remove_input_padding \\\n",
    "    --enable_context_fmha \\\n",
    "    --paged_kv_cache \\\n",
    "    --max_input_len 2048 --max_output_len 512 \\\n",
    "    --output_dir <<<<FIXME>>>> \\\n",
    "    --world_size <<<<FIXME>>>> \\\n",
    "    --tp_size <<<<FIXME>>>> \\\n",
    "    --pp_size <<<<FIXME>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION \n",
    "# 2) Build the engine \n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\\n",
    "    --model_dir $hf_weights_dir \\\n",
    "    --dtype float16 \\\n",
    "    --use_gpt_attention_plugin float16 \\\n",
    "    --use_gemm_plugin float16 \\\n",
    "    --use_rmsnorm_plugin float16 \\\n",
    "    --use_inflight_batching \\\n",
    "    --remove_input_padding \\\n",
    "    --enable_context_fmha \\\n",
    "    --paged_kv_cache \\\n",
    "    --max_input_len 2048 --max_output_len 512 \\\n",
    "    --output_dir $trt_engine_2gpus \\\n",
    "    --world_size 2 \\\n",
    "    --tp_size 2 \\\n",
    "    --pp_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Run and test your engine </br>\n",
    "!mpirun -n <<<<FIXME>>>> --allow-run-as-root python tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "    --engine_dir= <<<<FIXME>>>>\\\n",
    "    --max_output_len 128 \\\n",
    "    --tokenizer_dir <<<<FIXME>>>>\\\n",
    "    --input_text <<<<FIXME>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# 3) Run and test your engine </br>\n",
    "!mpirun -n 2 --allow-run-as-root python tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "    --engine_dir=$trt_engine_2gpus\\\n",
    "    --max_output_len 128 \\\n",
    "    --tokenizer_dir $hf_weights_dir \\\n",
    "    --input_text \"How do I count in French? 1 un \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Please proceed on to [Inference of the LLama 13B model with Triton Inference server and TensorRT-LLM as a backend.](04_TRTLLMAndTritonRunRemoteInferenceOfTheLlama.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
