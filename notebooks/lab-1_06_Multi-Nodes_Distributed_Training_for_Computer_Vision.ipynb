{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Multi-Nodes Distributed Training for Computer Vision\n",
    "\n",
    "In this notebook, we will learn how to train a simple Image Classifier in distributed mode.\n",
    "We will first implement a vanilla pipeline parallel distribution. Then, we will use Microsoft's [DeepSpeed Library](https://www.deepspeed.ai/) for model distribution and optimization techniques.\n",
    "\n",
    "## The goals\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Learn how to train a vanilla CNN\n",
    "* Port the code to DeepSpeed library \n",
    "* Scale training using Data parallel distribution\n",
    "* Optimize training with DeepSpeed autotuning and Zero Redundancy Optimizer\n",
    "\n",
    "\n",
    "**[5.1 Convolutional Neural Network for Image Classification on CIFAR-10](#1.1-The-hardware-overview)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.1.1 The dataset](#1.1.3-Check-The-Interconnect-Topology)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.1.2 Convolutional Neural Network](#1.1.3-Check-The-Interconnect-Topology)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.1.3 Naive Model Distribution](#1.1.3-Check-The-Interconnect-Topology)<br>\n",
    "**[5.2 Distributed Training with DeepSpeed](#1.1-The-hardware-overview)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.2.1 Make the code run with DeepSpeed](#1.1.3-Check-The-Interconnect-Topology)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.2.2 Scale-out training with Data Parallelism](#1.1.3-Check-The-Interconnect-Topology)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.2.3 Zero Redundancy Optimizer ](#1.1.3-Check-The-Interconnect-Topology)<br>\n",
    "\n",
    "### Cancel Previous Running/Pending Jobs\n",
    "\n",
    "Before moving on, check that no jobs are still running or waiting on the SLURM queue. Let's check the SLURM jobs queue by executing the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5.1 Convolutional Neural Network for Image Classification on CIFAR-10\n",
    "\n",
    "## 5.1.1 The dataset\n",
    "\n",
    "\n",
    "The [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html) consists of 60K images (50K for training and 10K for testing). Images are 32 by 32 pixels labelled with 10 classes: plane, car, bird, cat, deer, dog, frog, horse, ship and truck. \n",
    "In this lab, we will train a simple Convolutional Neural Network to classify CIFAR-10 images.\n",
    "\n",
    "To download the dataset, we will use the [Torchvision](https://pytorch.org/vision/stable/index.html) package, a Pytorch library that contains popular datasets, model architectures, and common image transformations for Computer Vision.\n",
    "<img src=\"images/CIFAR-10.jpg\" width=\"350\" />\n",
    "\n",
    "Let's first import the relevant libraries by executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import display_html\n",
    "\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "\n",
    "# define an image transform \n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next 2 cells to download the training and test CIFAR10 corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download the CIFAR10 training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the CIFAR10 test dataset\n",
    "testset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                       train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=64,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some random training images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(images, labels):\n",
    "    for i in range(8): \n",
    "        img = images[i] / 2 + 0.5\n",
    "        npimg = img.numpy()\n",
    "        plt.subplot(2,4,i+1)\n",
    "        plt.imshow(np.transpose(npimg, (1, 2 , 0)));\n",
    "        plt.axis('off');\n",
    "        plt.title(classes[labels[i]])\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# get some training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "# Show images\n",
    "imshow(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Convolutional Neural Network\n",
    "\n",
    "Let's define a neural network with 2 convolutional layers, first followed by a pooling layer then 3 fully connected layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "cnn_net = CNN_Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the model to GPU 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the Convolutional Neural Network\n",
    "from torchsummary import summary\n",
    "summary(cnn_net,input_size=(3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the optimizer and hyperparameters. We will use the Stochastic Gradient Descent (SGD) with momentum optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_net.parameters(), lr=0.001,momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# Tensorboard event recording directory \n",
    "writer = SummaryWriter('megatron/tensorboard/cifar10')\n",
    "\n",
    "log_interval=100\n",
    "batch_size=64\n",
    "epochs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN\n",
    "for epoch in range(epochs): \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = cnn_net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()     \n",
    "        \n",
    "        # print the loss and accuracy metrics log_interval mini-batches\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        if i % log_interval == (log_interval - 1):  \n",
    "            print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / log_interval, 100.*correct/total))\n",
    "            writer.add_scalar(\"Training Cross Entropy Loss\", running_loss / log_interval, i + 1)\n",
    "            writer.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
    "            running_loss = 0.0\n",
    "    # print the last iterations \n",
    "    print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / ((i % log_interval) + 1), 100.*correct/total))\n",
    "    writer.add_scalar(\"Training Cross Entropy Loss\", running_loss / ((i % log_interval) + 1), i + 1)\n",
    "    writer.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
    "\n",
    "print('Training Done')\n",
    "writer.add_graph(cnn_net, inputs)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Training and Inspect the Model on Tensorboard \n",
    "\n",
    "We set the arguments for recording Tensorboard events in the previous CNN training. The training loss is available under the name `cifar10`. We can also visualize the model and navigate throughout its layers on the \"Graphs\" tab.\n",
    "\n",
    "<img src=\"images/CNN.png\" width=\"750\"/>\n",
    "\n",
    "Execute the next cell to create a link to Tensorboard for your browser. Then, click the link to see graphs of experiment metrics saved in the specified `Tensorboard` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate The Trained Model\n",
    "\n",
    "Let's evaluate the trained model on the CIFAR-10 test dataset. Execute the following cell to evaluate the accuracy on the test set. Accuracy detail per class will also be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = cnn_net(images.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "        c = (predicted == labels.to(device)).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %2f %%' %\n",
    "      (100 * correct / total))\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2f %%' %\n",
    "          (classes[i], 100 * class_correct[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1.3 Naive Model Distribution\n",
    "\n",
    "Let's now implement a naive pipeline parallel distribution of the previous CNN model. To do so, we need to explicitly place each layer into the device and implement the forward pass, accordingly moving the corresponding outputs to match devices accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will run the CNN model on two GPUs by placing: conv1 + pooling on GPU0 and conv2 + fc1 + fc2+ fc3 on GPU1. \n",
    "We will use torch *[TORCH.TENSOR.TO](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to)* to convert tensors to cuda and place them to the desired device. \n",
    "\n",
    "Have a look at modified class on the defined `Net_Parallel` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net_Parallel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_Parallel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5).to('cuda:0')               # Changed here\n",
    "        self.pool = nn.MaxPool2d(2, 2).to('cuda:0')                # Changed here\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5).to('cuda:1')              # Changed here\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120).to('cuda:1')         # Changed here\n",
    "        self.fc2 = nn.Linear(120, 84).to('cuda:1')                 # Changed here\n",
    "        self.fc3 = nn.Linear(84, 10).to('cuda:1')                  # Changed here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x.to('cuda:0'))))          # Changed here\n",
    "        x = self.pool(F.relu(self.conv2(x.to('cuda:1'))))          # Changed here\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "cnn_net_pp = Net_Parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous training execution, run the next 2 cells to define the optimizer/hyperparameters and then launch the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_net_pp.parameters(), lr=0.001,momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "# Tensorboard event recording directory \n",
    "writer_pp = SummaryWriter('megatron/tensorboard/cifar10_PP')                 \n",
    "\n",
    "log_interval=100\n",
    "batch_size=64\n",
    "epochs=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the GPUs activity by running the next cell to open a terminal and the watch `nvidia-smi` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Check the GPUs: <font color=\"green\">watch nvidia-smi</font>\n",
    "</pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the terminal window opened and execute the next training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN with pipeline parallel\n",
    "for epoch in range(epochs): \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = cnn_net_pp(inputs.to('cuda:0'))                                # Changed here\n",
    "        loss = criterion(outputs, labels.to('cuda:1'))                           # Changed here\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.reset_max_memory_allocated(0)\n",
    "        # print the loss and accuracy metrics log_interval mini-batches\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.to('cuda:1')).sum().item()                 # Changed here\n",
    "        if i % log_interval == (log_interval - 1):  \n",
    "            print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / log_interval, 100.*correct/total))\n",
    "            writer_pp.add_scalar(\"Training Cross Entropy Loss\", running_loss / log_interval, i + 1)\n",
    "            writer_pp.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
    "            running_loss = 0.0\n",
    "    # print the last iterations \n",
    "    print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / ((i % log_interval) + 1), 100.*correct/total))\n",
    "    writer.add_scalar(\"Training Cross Entropy Loss\", running_loss / ((i % log_interval) + 1), i + 1)\n",
    "    writer.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
    "\n",
    "print('Training Done')\n",
    "writer_pp.add_graph(cnn_net_pp, inputs)\n",
    "writer_pp.flush()\n",
    "writer_pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Distributed Training with DeepSpeed\n",
    "\n",
    "The DeepSpeed library is a Deep Learning optimization library for distributed training on several different hardware sizes ranging from a single GPU to low-end clusters, going up to massive supercomputers.\n",
    "\n",
    "<img src=\"https://www.deepspeed.ai/assets/images/3d-parallelism.png\" width=\"650\" />\n",
    "\n",
    "\n",
    "- Distributed Training with Mixed Precision on Single-GPU/Multi-GPU/Multi-Node\n",
    "- Pipeline Parallelism and an integration with Megatron-LM tensor parallel \n",
    "- Zero Redundancy Optimizer (ZeRO): Memory optimization techniques\n",
    "- ZeRO-Offload: Offloading data and compute to the CPU\n",
    "- Mixture of Experts (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 Make the code run with DeepSpeed\n",
    "\n",
    "The DeepSpeed engine can wrap neural networks defined as `torch.nn.module`. To write the previous Model with DeepSpeed, we will need to:\n",
    "\n",
    "- Instantiate the DeepSpeed Model engine and optimizer: Use the DeepSpeed initializer as follows: \n",
    "`model_engine, optimizer= deepspeed.initialize(args=args, model=Your_Network, model_parameters=parameters, training_data=trainset)`\n",
    "- The variable `args` should embed training arguments and the DeepSpeed arguments. We can use `deepspeed.add_config_arguments(parser)` to add the DeepSpeed arguments to our parser.\n",
    "- Refer to the deepspeed `model_engine` instead of `Your_Network` in the rest of the code.\n",
    "- For distributed training implementation with `torch.distributed.init_process_group(...)`, replace that with `deepspeed.init_distributed()`\n",
    "\n",
    "Learn more on how to write DeepSpeed models with the [dedicated DeepSpeed documentation.](https://www.deepspeed.ai/getting-started/#writing-deepspeed-models) \n",
    "\n",
    "DeepSpeed arguments can be passed throughout a JSON configuration file. Let's have a look at our the DeepSpeed configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the DeepSpeed config\n",
    "!cat code/moe/ds_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant library\n",
    "import deepspeed\n",
    "\n",
    "# define the argument class, the training arguments, and DeepSpeed\n",
    "class Args:\n",
    "    log_interval=100 \n",
    "    batch_size=64\n",
    "    epochs=2\n",
    "    deepspeed = True\n",
    "    deepspeed_config = \"code/moe/ds_config.json\"\n",
    "    local_rank= 0\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the CNN network\n",
    "cnn_net_ds = CNN_Net()\n",
    "\n",
    "# Define the hyperparameters\n",
    "parameters = filter(lambda p: p.requires_grad, cnn_net_ds.parameters())\n",
    "\n",
    "# Wrap the CNN network with DeepSpeed\n",
    "model_engine, optimizer, _, _ = deepspeed.initialize(args=args, model=cnn_net_ds, model_parameters=parameters, training_data=trainset)\n",
    "\n",
    "# enable mixed precision\n",
    "fp16 = model_engine.fp16_enabled()\n",
    "\n",
    "device = model_engine.local_rank\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Tensorboard event recording directory \n",
    "writer_ds = SummaryWriter('megatron/tensorboard/cifar10_DS')                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN with DeepSpeed\n",
    "for epoch in range(epochs): \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        if fp16:\n",
    "            inputs = inputs.half()        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_engine(inputs)             # Changed net_cnn to model_engine\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        model_engine.backward(loss)                # Changed net_cnn to model_engine\n",
    "        model_engine.step()                        # Changed net_cnn to model_engine\n",
    "        \n",
    "        # print the loss and accuracy metrics log_interval mini-batches\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        if i % log_interval == (log_interval - 1):  \n",
    "            print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / log_interval, 100.*correct/total))\n",
    "            writer_ds.add_scalar(\"Training Cross Entropy Loss\", running_loss / log_interval, i + 1)\n",
    "            writer_ds.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    # print the last iterations \n",
    "    print('[epoch %d, iterations %5d] loss: %.3f accuracy: %2f %%' %  (epoch , i + 1, running_loss / ((i % log_interval) + 1), 100.*correct/total))\n",
    "    writer.add_scalar(\"Training Cross Entropy Loss\", running_loss / ((i % log_interval) + 1), i + 1)\n",
    "    writer.add_scalar(\"Training Accuracy\", 100.*correct/total, i + 1)\n",
    "    \n",
    "print('Training Done')\n",
    "writer_ds.add_graph(model_engine, inputs)\n",
    "writer_ds.flush()\n",
    "writer_ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous steps are aggregated into the python script [cifar10_deepspeed.py](./code/moe/cifar10_deepspeed.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 Scale-out training with Data Parallelism \n",
    "\n",
    "Let's now scale our previous training using distributed training techniques. To run the previous code on 4 GPUs using Data Parallel Distribution, instead of using torch.distributed.launch command, we can use deepspeed implementation.\n",
    "\n",
    "`python -m torch.distributed.launch --nproc_per_node=4 my_code.py <args>`\n",
    "\n",
    "With DeepSpeed, simply replace torch.distributed.launch with deepspeed and add a new argument --deepspeed ds_config.json\n",
    "\n",
    "`deepspeed --num_gpus=4 my_code.py  <args> --deepspeed ds_config.json`\n",
    "\n",
    "Execute the next cell to run the previous cifar10_deepspeed.py training code on 4 GPUs with DeepSpeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill zombie processes\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the training on 4 GPUs with Data parallel\n",
    "!deepspeed --num_gpus=4 /dli/code/moe/cifar10_deepspeed.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/code/moe/ds_config.json \\\n",
    "    --profile-execution=True \\\n",
    "    --profile-name='zero0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the same run on a SLURM based cluster, simply write the SBATCH script allocating the desired resources and invoke the same DeepSpeed command. \n",
    "\n",
    "Run the next cell to create the SBATCH script allocating 2 nodes to run the previous training with DeepSpeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /dli/code/run_cifar10_deepspeed_2Nodes.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_ds\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "# Number of nodes\n",
    "NUM_NODES=2\n",
    "# Number of GPUs per node\n",
    "NUM_GPUS=2\n",
    "\n",
    "\n",
    "deepspeed --num_nodes=${NUM_NODES} --hostfile /dli/code/moe/hostfile --num_gpus=${NUM_GPUS} /dli/code/moe/cifar10_deepspeed.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/code/moe/ds_config.json \\\n",
    "    --profile-execution=True \\\n",
    "    --profile-name='zero0_sbatch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [run_cifar10_deepspeed_2Nodes.sh](./code/run_cifar10_deepspeed_2Nodes.sh) and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the 2 nodes jobs\n",
    "!sbatch /dli/code/run_cifar10_deepspeed_2Nodes.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU utilization on the master node\n",
    "!sleep 10\n",
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 Zero Redundancy Optimizer \n",
    "\n",
    "[ZeRO](https://www.deepspeed.ai/tutorials/zero/) reduces the memory consumption of each GPU by distributing the model's training states (weights, gradients, and optimizer states) across the available devices. ZeRO is implemented as 3 accumulative stages:\n",
    "\n",
    "- **ZeRO-1:** The optimizer states are partitioned across the processes, so that each process updates only its partition. For instance, the Adam optimizer will store 32-bit weights, and the first, and second moment estimates.\n",
    "- **ZeRO-2:** In addition to Zero 1, the reduced 32-bit gradients for updating the model weights are also partitioned such that each process retains only the gradients corresponding to its portion of the optimizer states.\n",
    "- **ZeRO-3:** In addition to Zero 2, the 16-bit model parameters are partitioned across the processes. ZeRO-3 will automatically collect and partition them during the forward and backward passes. ZeRO-3 also includes the [ZeRO-Infinity](https://arxiv.org/pdf/2104.07857.pdf) engine that offloads to both CPU and NVMe memory for memory savings.\n",
    "\n",
    "```\n",
    "{\n",
    " \"zero_optimization\": {\n",
    "    \"stage\": [0|1|2|3],\n",
    "    \"allgather_partitions\": [true|false],\n",
    "    \"allgather_bucket_size\": 5e8,\n",
    "    \"overlap_comm\": false,\n",
    "    \"reduce_scatter\": [true|false],\n",
    "    \"reduce_bucket_size\": 5e8,\n",
    "    \"contiguous_gradients\" : [true|false],\n",
    "    \"offload_param\": {\n",
    "      ...\n",
    "    },\n",
    "    \"offload_optimizer\": {\n",
    "      ...\n",
    "    },\n",
    "    \"stage3_max_live_parameters\" : 1e9,\n",
    "    \"stage3_max_reuse_distance\" : 1e9,\n",
    "    \"stage3_prefetch_bucket_size\" : 5e8,\n",
    "    \"stage3_param_persistence_threshold\" : 1e6,\n",
    "    \"sub_group_size\" : 1e12,\n",
    "    \"elastic_checkpoint\" : [true|false],\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": [true|false],\n",
    "    \"ignore_unused_parameters\": [true|false]\n",
    "    \"round_robin_gradients\": [true|false]\n",
    "    }\n",
    "  }\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/02/Turing-Animation.mp4?_=1\" width=\"560\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to illustrate the advantages of ZeRO optimizer in memory saving, let's scale our CNN model to a larger neural network. We  replaced the tiny CNN network with the Resnet152 model with 11 million parameters in the [large_model_deepspeed.py](./code/moe/large_model_deepspeed.py).\n",
    "\n",
    "\n",
    "The following figure shows the CPU usage via [htop](https://htop.dev/) command and the GPU 0 memory profiled using Pytorch Profiler (available on the Tensorboard) for the *resnet152* training step. We can see a comparison between no ZeRO and ZeRO stage 3 + Offload. The memory peak required per GPU without ZeRO optimizer is about 1.5GB, while offloading the parameters to the CPU with ZeRO-3 uses only 845MB. On the other hand, the time per step increased significantly from 500ms to 7000ms!\n",
    "\n",
    "ZeRO infinity allows sacling to larger models when the number of GPUs are limited, but this comes with an additional training time corresponding to data movement.\n",
    "<img src=\"images/zero_memory.png\" width=\"1250\" />\n",
    "\n",
    "\n",
    "Let's run the Zero stage 3 optimizer and enable CPU offload of the parameters. We have prepared the configuration file. Notice that we have increased the training batch size per GPU to 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DeepSpeed config file for Zero stage 3 Offload\n",
    "!cat /dli/code/moe/ds_config_stage_3.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the resnet152 model. More information about it can be found [here](https://pytorch.org/hub/pytorch_vision_resnet/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "net = torch.hub.load(\"pytorch/vision:v0.10.0\", \"resnet152\", force_reload=True, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell to run the *resnet152* model training on the CIFAR-10 dataset. Notice that in these experiments, our focus is memory consumption as opposed to model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!deepspeed --num_gpus=4 /dli/code/moe/large_model_deepspeed.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/code/moe/ds_config_stage_3.json \\\n",
    "    --profile-execution=True \\\n",
    "    --profile-name='zero_resnet152_stage3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Execute the next cell to create a link to Tensorboard in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: ZeRO Stage 1\n",
    "\n",
    "Compare ZeRO Stage 1 execution time and GPU/CPU memory to no ZeRO and ZeRO Stage 3 + Offload. We have already prepared the DeepSpeed configuration file. Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DeepSpeed config file for Zero stage 1 \n",
    "!cat /dli/code/moe/ds_config_stage_1.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell to run the *resnet152* model training on the CIFAR-10 dataset with ZeRO Stage1. Replace the `FIXME` with the corresponding arguments. If you get stuck, you can look at the [solution](solutions/ex5.2.4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepspeed --num_gpus=4 /dli/code/moe/large_model_deepspeed.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config #FIXEME \\\n",
    "    --profile-execution=True \\\n",
    "    --profile-name=#FIXEME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the run without ZeRO, Stage 1 allows to reduce the GPU memory usage from about 1.5GB to 1GB as it partitions the optimizer states across 4 GPUs meaning that each GPU is responsible for keeping in memory a part of the optimizer states and communicate them when necessary to others. In this case, no significant extra time is observed to run the step with ZeRO Stage 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: 5.2.4  Autotuning\n",
    "\n",
    "When designing training experiments, it is hard to know what are the best configurations (such as micro-batch size) that fully utilize the hardware and achieve a high throughput number. Usually, configuration exploration is performed manually. This tuning process can be painful, time-consuming and the configurations are hardware-dependent. The DeepSpeed Autotuner automatically discovers the optimal DeepSpeed configuration parameters that optimizes for training speed.\n",
    "To launch the autotuning, add `--autotuning run` command to the training script and enable it on the configuration file as follows: \n",
    "\n",
    "```\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "\n",
    "  \"autotuning\": {\n",
    "    \"enabled\": true,\n",
    "    \"arg_mappings\": {\n",
    "      \"train_micro_batch_size_per_gpu\": \"--batch_size\"    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "The arg_mappings dictionary autotuning section of the DeepSpeed configuration file provides the naming mappings between the parameters in DeepSpeed configuration and the training script arguments. It is also possible restrict the search space to a list of values by replacing \"auto\" with a vector of possible values to be evaluated. \n",
    "\n",
    "To learn more about the Autotuning feature, check the [DeepSpeed documentation](https://www.deepspeed.ai/tutorials/autotuning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Before moving on, we need to make sure no jobs are still running or waiting in the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the admin user's jobs using the `scancel` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will see Mixture of Expert (MOE)configurations. Move on to [06_MOE_alternative_models.ipynb](06_MOE_alternative_models.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
