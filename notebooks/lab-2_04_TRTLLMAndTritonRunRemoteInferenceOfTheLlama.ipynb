{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Llama2 deployment with NVIDIA TensorRT-LLM and Triton Inference server \n",
    "\n",
    "In the previous section, we have seen how to take an unoptimized model and significantly accelerate it using TensorRT-LLM. This is significant, as it allows us to dramatically reduce the size of the capital investment needed to deploy an application as well as the ongoing operational costs. TensorRT-LLM can frequently be the difference between a non-feasible idea and a product that generates revenue. At the end of the previous notebook, we only had a binary with no real means of serving it to large groups of users. We did not have a way to deploy this binary to production, not to mention, to manage the efficiency of its execution. We will address the above and have an application that is close to being ready for production. \n",
    "\n",
    "In this notebook we will: \n",
    "* Discuss key features of NVIDIA Triton inference server and how it can be integrated with the TensorRT-LLM Execution Backend. \n",
    "* Learn how to prepare the LLama2 for deployment using Triton. \n",
    "* Run Triton inference server with Llama2 using a two-way Tensor Parallel deployment. \n",
    "* We will write a client application using our model hosted on Triton using Python client API `tritonclient` \n",
    "\n",
    "\n",
    "**[4.1 NVIDIA Triton Inference Server](#4.1)<br>** \n",
    "**[4.2 Overview of the end-to-end inference pipeline](#4.2)<br>** \n",
    "**[4.3 Preparing the inference environment](#4.3)<br>** \n",
    "**[4.4 Triton Inference Server configuration](#4.4)<br>** \n",
    "**[4.5 Launching Triton Inference Server](#4.5)<br>** \n",
    "**[4.6 Writing client application consuming our LLama2 model](#4.6)<br>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 NVIDIA Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA Triton™ Inference Server, part of the NVIDIA AI platform, is an open-source inference serving software that helps standardize model deployment and execution and delivers fast and scalable AI in production. Triton streamlines AI inference by enabling teams to deploy, run, and scale trained AI models from any framework on any GPU- or CPU-based infrastructure. It provides AI researchers and data scientists the freedom to choose the right framework for their projects without impacting production deployment. It also helps developers deliver high-performance inference across cloud, on-prem, edge, and embedded devices. \n",
    "<div style=\"text-align:center\"> \n",
    "<img src=\"./images/TRTLLM_custom_backend.PNG\" style=\"width: 60%;position:relative;\" /> \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triton inference server is distributed with a number of execution backends including backends supporting TensorRT, PyTorch, custom Python/C++ backend, HugeCTR, OpenVion and many other including TensorRT-LLM. It can be extended by developers as new backends can also be integrated. [TensorRT-LLM execution backend](https://github.com/triton-inference-server/tensorrtllm_backend) written by NVIDIA allows to deploy models optimized using TensorRT-LLM library not only in a single GPU but also multi-GPU and multi node mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Overview of the end-to-end inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the Triton deployment process, let us take a bird's eye view of the end-to-end deployment process. We just need now to focus on configuring Triton Inference server and its execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/TRTLLM_Triton_pipeline_nospell.png\" style=\"width: 70%;position:relative;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Preparing the inference environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already downloaded `tensorrtllm_backend` code from GitHub (https://github.com/triton-inference-server/tensorrtllm_backend), so we will be able to use Triton's configs for LLama2 prepared by NVIDIA. They are placed in <code>tensorrtllm_backend/all_models/inflight_batcher_llm</code>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration file is a perfect example of model ensemble in Triton Inference Server. Triton is a general purpose inference server allowing not only to run individual neural networks, but also construct complex ensembles/pipelines. These pipelines may contain multiple neural network stages connected with each other into one inference service. Pipelines can be composed of different components. One can add Python/C++ scripts before and/or after any neural network to add any pre/post processing steps that could transform the data into its final format.  </br> \n",
    "\n",
    "In the `LLama2` inference pipeline we have three distinct execution steps: </br> `preprocessing` -> `model execution with tensorrtllm` -> `postprocessing` </br>  \n",
    "\n",
    "* preprocessing: tokenize the input text (Python Backend)\n",
    "\n",
    "* tensorrtllm: infer the TRTLLM engine (TensorRT-LLM backend)\n",
    "\n",
    "* postprocessing: decode the text output (Python Backend)\n",
    "\n",
    "* ensemble folder describing the Inputs/Outputs and the sequence of models to call during inference.\n",
    "\n",
    "When querying the TensorRT-LLM model, we will query only the \"ensemble\" which is responsible for all the pipeline. Overall, the client-server inference scheme looks like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/Triton_execution.png\" style=\"width: 30%;position:relative;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's copy all the template congifuration files in our own \"Model_repository\" folder\n",
    "%cd tensorrtllm_backend/all_models/inflight_batcher_llm/\n",
    "!cp -r ensemble preprocessing tensorrt_llm postprocessing /dli/task/model_repository   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy our 4 GPUs TRTLLM engines to the prepared Triton model_repository/tensorrt_llm/1/\n",
    "%cd /dli/task\n",
    "!cp trt-engines/llama_13b/fp16/4-gpus/* model_repository/tensorrt_llm/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Triton Inference Server configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating model configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Let's edit the Triton configuration files responsible for the Llama2 model inference. <br/> \n",
    "\n",
    "Start with the config file in the TensorRT-LLM repository: <b>[model_repository/tensorrt_llm/config.pbtxt](model_repository/tensorrt_llm/config.pbtxt)</b> <br/>\n",
    "\n",
    "1) Please update `gpt_model_path` value with the path to our TensorRT-LLM engines (4GPUs) in the model_repository.<br/> \n",
    "parameters: {<br/> \n",
    "  key: \"gpt_model_path\"<br/> \n",
    "  value: {<br/> \n",
    "    string_value: \"<<< FIXME >>>\"<br/> \n",
    "  }<br/> \n",
    "}<br/> \n",
    "\n",
    "2) Please update the `model_transaction_policy` decoupled to \"False\".<br/> \n",
    "model_transaction_policy {<br/> \n",
    "  decoupled: \"<<< FIXME >>>\"<br/> \n",
    "}<br/> \n",
    "\n",
    "We also need to define the Tokenizer repository in the preprocessing and post processing step of the ensemble.<br/>\n",
    "Let's modify the config file for the preprocessing step <b>[model_repository/preprocessing/config.pbtxt](model_repository/preprocessing/config.pbtxt)</b> <br/>\n",
    "\n",
    "1) Please update the `tokenizer_dir` to the Llama tokenizer <br/>\n",
    "parameters {<br/> \n",
    "  key: \"tokenizer_dir\"<br/> \n",
    "  value: {<br/> \n",
    "    string_value: \"<<< FIXME >>>\" <br/> \n",
    "  }<br/> \n",
    "}<br/> \n",
    "\n",
    "2) Please update the `tokenizer_type` to \"llama\" <br/>\n",
    "parameters {<br/> \n",
    "  key: \"tokenizer_type\"<br/> \n",
    "  value: {<br/> \n",
    "    string_value: \"<<< FIXME >>>\" <br/> \n",
    "  }<br/> \n",
    "}<br/> \n",
    "\n",
    "Finally let's modify the config file for the postprocessing step <b>[model_repository/postprocessing/config.pbtxt](model_repository/postprocessing/config.pbtxt)</b><br/>\n",
    "\n",
    "1) Please update the `tokenizer_dir` to the Llama tokenizer <br/>\n",
    "parameters {<br/> \n",
    "  key: \"tokenizer_dir\"<br/> \n",
    "  value: {<br/> \n",
    "    string_value: \"<<< FIXME >>>\" <br/> \n",
    "  }<br/> \n",
    "}<br/> \n",
    "\n",
    "2) Please update the `tokenizer_type` to \"llama\" <br/>\n",
    "parameters {<br/> \n",
    "  key: \"tokenizer_type\"<br/> \n",
    "  value: {<br/> \n",
    "    string_value: \"<<< FIXME >>>\" <br/> \n",
    "  }<br/> \n",
    "}<br/> \n",
    "\n",
    "\n",
    "If you have a doubt, please check the solutions files :\n",
    "- [tensorrt_llm](solutions/config_tensorrtllm.pbtxt)\n",
    "- [preprocessing](solutions/config_preprocessing.pbtxt)\n",
    "- [postprocessing](solutions/config_postprocessing.pbtxt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Launching Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now bring all the parts together and start Triton hosting our LLama2 inference pipeline: </br> \n",
    "1. We have built and started Triton docker container with the TensorRT-LLM backend that supports distributed inference of Llama</br> \n",
    "2. We have downloaded model weights and converted them into TensorRT-LLM format. </br> \n",
    "3. We have prepared the Triton configuration file 'config.pbtxt'.</br> \n",
    "\n",
    "We are ready to start Triton Inference Server! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command that we will launch is described in the python script [launch_triton_server.py](https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/scripts/launch_triton_server.py).</br>\n",
    "It is this command line, adapted for multiple GPU serving. </br>\n",
    "`mpirun --allow-run-as-root -n 1 tritonserver --grpc-port 8001 --metrics-port 8002 --model-repository=/dli/task/model_repository &`</br>\n",
    "Let us look at the command segment one at a time:\n",
    "-  <code>mpirun --allow-run-as-root -n 1 </code> We uses `mpirun``, a command that will allow us to execute Triton on multiple GPUs. In this case, the parameter `-n`` indicates that the number of gpus is set to 1, meaning only one gpu will be used. One could specify a higher number, deploying across multiple servers in our datacenter. \n",
    "- <code>--http-port 8000 --grpc-port 8001 --metrics-port=8002 </code>open the HTTP (8000) and GRPC (8001) ports to handle requests and port 8002 to log the metrics \n",
    "- <code>--model-repository=some_path</code>, which points to the directory where we have placed configuration files for all models that Triton will be serving (Triton can easily serve multiple models/pipelines). Triton will automatically scan through this folder, find model configurations, load them onto the GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We will start Triton in an external Terminal window. All output logs will be printed into the external command window output. In a Jupyter notebook, this would stop at this command until Triton is terminated.</b></br>\n",
    "If Triton starts successfully, we will see these lines in the output. </br>\n",
    "It will display information about the ensemble of Llama stages that were found by the Triton in our model directory: </br>\n",
    "<code>\n",
    "+-------------------+---------+--------+</br>\n",
    "| Model             | Version | Status |</br>\n",
    "+-------------------+---------+--------+</br>\n",
    "| ensemble          | 1       | READY  |</br>\n",
    "| tensorrtllm       | 1       | READY  |</br>\n",
    "| postprocessing    | 1       | READY  |</br>\n",
    "| preprocessing     | 1       | READY  |</br>\n",
    "+-------------------+---------+--------+</br></code>\n",
    "</br>\n",
    "It will also display information about successful service initialization:</br>\n",
    "<code>\n",
    "I0503 17:26:25.226719 1668 grpc_server.cc:4421] Started GRPCInferenceService at 0.0.0.0:8001</br>\n",
    "I0503 17:26:25.227017 1668 http_server.cc:3113] Started HTTPService at 0.0.0.0:8000</br>\n",
    "I0503 17:26:25.283046 1668 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch the triton server, we will run this python script from a terminal. </br>\n",
    "<b>python tensorrtllm_backend/scripts/launch_triton_server.py --world_size=4 --model_repo=./model_repository </b>\n",
    "\n",
    "**TODO**\n",
    "1) Please exectute the Cell below and click on the Terminal link.\n",
    "2) Then Copy and Paste the above Python command line in the terminal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%html\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Launch Triton with this command: <font color=\"green\">python tensorrtllm_backend/scripts/launch_triton_server.py --world_size=4 --model_repo=./model_repository </font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Writing client applications consuming our Llama model\n",
    "\n",
    "In this section, we will go through several methods to send requests to our Triton Server. </br>\n",
    "4.6.1 will use `Curl` and `Generate endpoint` to send prompts requests through HTTP </br>\n",
    "4.6.2 will use an example of existing Python Client script </br>\n",
    "4.6.3 will demonstrate how to create your own python Client script </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.1 Query the server with the Triton generate endpoint\n",
    "Starting with Triton 23.10 release, you can query the server using Triton's [generate endpoint](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_generate.md) with a curl command based on the following general format within your client environment/container:</br>\n",
    "<code>curl -X POST localhost:8000/v2/models/${MODEL_NAME}/generate -d '{\"{PARAM1_KEY}\": \"{PARAM1_VALUE}\", ... }'</code></br>\n",
    "\n",
    "In the case of the models used in this example, you can replace MODEL_NAME with ensemble. Examining the ensemble model's config.pbtxt file, we can see that 4 parameters are required to generate a response for this model:\n",
    "- \"text_input\": Input text to generate a response from</br>\n",
    "- \"max_tokens\": The number of requested output tokens</br>\n",
    "- \"bad_words\": A list of bad words (can be empty)</br>\n",
    "- \"stop_words\": A list of stop words (can be empty)</br>\n",
    "\n",
    "Therefore, we can query the server in the following way:</br>\n",
    "<code>curl -X POST localhost:8000/v2/models/ensemble/generate -d '{\"text_input\": \"Answer the question: What is machine learning?\", \"max_tokens\": 32, \"bad_words\": \"\", \"stop_words\": \"\"}'</code></br>\n",
    "if using the ensemble model. \n",
    "\n",
    "Which should return a result similar to (formatted for readability):</br>\n",
    "{</br>\n",
    "  \"model_name\": \"ensemble\",</br>\n",
    "  \"model_version\": \"1\",</br>\n",
    "  \"sequence_end\": false,</br>\n",
    "  \"sequence_id\": 0,</br>\n",
    "  \"sequence_start\": false,</br>\n",
    "  \"text_output\": \"Answer the question: What is machine learning?\\nMachine learning is a branch of artificial intelligence that uses statistical techniques to give computer systems the ability to “learn” (i.e., progressively improve performance\"\\n\\n\"</br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST localhost:8000/v2/models/ensemble/generate -d '{\"text_input\": \"Answer the question: What is machine learning?\", \"max_tokens\": 32, \"bad_words\": \"\", \"stop_words\": \"\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.2 Utilize the provided client script to send a request\n",
    "You can also send requests to the \"ensemble\" model with the provided [python client script](tensorrtllm_backend/tools/inflight_batcher_llm/end_to_end_streaming_client.py) as following, which is using the Tritonclient interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tensorrtllm_backend/tools/inflight_batcher_llm/end_to_end_streaming_client.py --prompt 'Born in north-east France, Soyer trained as a' --output_len 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.3 Write your own client script\n",
    "Let's see more in details how to send requests using tritonclient.</br>\n",
    "To create an instance of the `client`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import np_to_triton_dtype\n",
    "\n",
    "import numpy as np\n",
    "client = grpcclient.InferenceServerClient(\"localhost:8001\",\n",
    "                                           verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then implement a function `prepare inputs` that prepares the necessary inputs to infer our Llama model, as expected and documented in the ensemble Config.pbtxt file. </br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tensor(name, input):\n",
    "    t = grpcclient.InferInput(name, input.shape,\n",
    "                              np_to_triton_dtype(input.dtype))\n",
    "    t.set_data_from_numpy(input)\n",
    "    return t\n",
    "\n",
    "def prepare_inputs(prompt,desired_output_len):\n",
    "    input0 = [[prompt]]\n",
    "    input_data = np.array(input0).astype(object)\n",
    "    output_len = np.ones_like(input0).astype(np.uint32) * desired_output_len\n",
    "    bad_words_list = np.array([[\"\"]], dtype=object)\n",
    "    stop_words_list = np.array([[\"\"]], dtype=object)\n",
    "    streaming = [[False]]\n",
    "    streaming_data = np.array(streaming, dtype=bool)\n",
    "\n",
    "    inputs = [\n",
    "        prepare_tensor(\"text_input\", input_data),\n",
    "        prepare_tensor(\"max_tokens\", output_len),\n",
    "        prepare_tensor(\"bad_words\", bad_words_list),\n",
    "        prepare_tensor(\"stop_words\", stop_words_list)\n",
    "    ]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the request to the server and get back the result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time, \"\n",
    "desired_output_len = 128\n",
    "inputs = prepare_inputs(prompt,desired_output_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.infer(\"ensemble\", inputs)\n",
    "output0 = result.as_numpy(\"text_output\")\n",
    "print(output0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.7 Measure Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "execution_time = 0\n",
    "num_iterations = 10\n",
    "for _ in range(num_iterations):\n",
    "    start = time.time()\n",
    "    output = client.infer(\"ensemble\", inputs)\n",
    "    end = time.time()\n",
    "    execution_time += end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average inference time of 128 tokens is:\",\n",
    "     1000 * (execution_time/float(num_iterations)), \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the 4xA100 setup this course was built with, the average inference time of 128 tokens is around 1.5s. \n",
    "So, we got an acceleration of around 2.6x, which is a starting point but far from being the best we can achieve with TRTLLM. \n",
    "With some advanced parameters like Inflight Batching, you could even reach 9x acceleration like in the plot below. \n",
    "\n",
    "Now you know how to run large models in production, and Triton Inference Server allows you to access your model from any device in the world! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/TRTLLM_performances.png\" style=\"width: 60%;position:relative;\"><br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Congratulations on finishing the course! If you're ready to put your skills to the test, please move on to the assessment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
