{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.4 The Training Performance\n",
    "\n",
    "In order to train large Neural Networks in a reasonable time, scaling the infrastructure is unavoidable. \n",
    "Let's first compute the number of parameters of our transformer models and estimate its training according to the hardware infrastructure and the experimentally observed training throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Number of Parameters of Transformers Model\n",
    "\n",
    "The number of parameters for a Transformers model is computed as:\n",
    "$P = 12 l h^2 (1 + \\frac{13}{12h} + \\frac{V+s}{12lh})$ where:\n",
    "- $l$ = Number of Layers\n",
    "- $h$ = Hidden Size\n",
    "- $V$ = Vocabulary Size\n",
    "- $s$ = Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parameters of the Transformers model\n",
    "def calculate_number_parameters(l,h,s,V):\n",
    "    # Compute the number of parameters of the model\n",
    "    P=12*l*h*h *(1+ (13/(12*h)) + ((V+s)/(12*l*h)))\n",
    "    print(\"The number of parameters for the GPT architecture is: {} \\n\".format(int(P)))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's compute the number of parameters of the transformer model having 40 layers, a hidden size of 6144, vocabulary size of 50257 and sequence length of 1024. This model should be approximately 18 billion parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model architecture parameters\n",
    "l=40\n",
    "h=6144\n",
    "s=1048\n",
    "V=50257\n",
    "    \n",
    "P=calculate_number_parameters(l,h,s,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Theoretical Peak FLOP per second per GPU\n",
    "\n",
    "As detailed in the paper [Scaling Language Model Training to a Trillion Parameters Using Megatron](https://arxiv.org/pdf/2104.04473.pdf), the majority of floating-point operations in the model are performed in the matrix multiplications (GEMMs). If we consider only these GEMMs operations, the number of FLOPs per iteration is $F = 96 B s l h^2 (1 + \\frac{s}{6h} + \\frac{V}{16lh})$ where $B$ is the batch size. \n",
    "\n",
    "And, in case we have an estimate of the time spent per iteration `Time_per_iteration_second`, it is possible then to compute the theoretical peak FLOP per second and per GPUs and estimate the GPU usage by comparing. \n",
    "\n",
    "The following table shows the training performance of several GPT model sizes (from 1.7B to 1 trillion) pretrained using Megatron-LM library on a SuperPOD cluster with A100 GPUs.\n",
    "<img src=\"https://github.com/NVIDIA/Megatron-LM/blob/main/images/cases_april2021.png?raw=true\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical peak FLOP per second per GPU - with activation checkpointing (2 forwards and 1 backward)\n",
    "def calculate_Theoretical_peak_FLOP_s_GPU(B,s, l,h,number_GPUs,Time_per_iteration_second):\n",
    "    # The number of FLOPs per iteration\n",
    "    F = 96*B*s* l*h*h *(1 + s/ (6*h) + V/(16*l*h))/1e+12\n",
    "    \n",
    "    #Theoretical peak FLOP per second per GPU\n",
    "    PF= (F/Time_per_iteration_second/number_GPUs)\n",
    "    print(\"Theoretical peak FLOP/s/GPU: {}\\n\".format(PF))\n",
    "    \n",
    "    # Percentage of theoretical peak FLOP/s on a A100 FP16 (change according the hardware)\n",
    "    GPU_usage= PF/ 312 *100\n",
    "    print(\"Percentage of theoretical peak FLOP/s: {}%\".format(GPU_usage))\n",
    "    \n",
    "    return PF, GPU_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of theoretical peak FLOP/s in the previous function is based on **A100** hardware capabilities in **FP16/BF16 which is 312**. This needs to be updated according to the corresponding Tensor Core GPU performance specs. To learn more about the [Ampere architecture specifications](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/).\n",
    "\n",
    "\n",
    "If we consider our previous 18B parameters models pretrained on 16 GPUs with a global batch size of 512, they have a time per iteration of 32.09s. \n",
    "Let's compute the theoretical peak FLOP per second per GPU and the percentage GPU utilization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size=512\n",
    "number_GPUs=16\n",
    "Time_per_iteration_second=32.09\n",
    "\n",
    "# Considering the 18B parameters model\n",
    "l=40\n",
    "h=6144\n",
    "s=1048\n",
    "V=50257\n",
    "    \n",
    "PF,GPU_usage=calculate_Theoretical_peak_FLOP_s_GPU(global_batch_size,s, l,h,number_GPUs,Time_per_iteration_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the Training Duration / Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to estimate training duration per epoch according to the model, dataset, and hardware size. Training time (in seconds) is approximated with this equation $\\approx \\frac{8*T*P}{n * PF}$ where: \n",
    "- $T$ = Number of tokens in the dataset\n",
    "- $P$ = Numbers of parameters \n",
    "- $n$ = Number of GPUs\n",
    "- $PF$ = Achieved teraFLOP/s per GPU\n",
    "\n",
    "More details are described in the paper [Scaling Language Model Training to a Trillion Parameters Using Megatron](https://arxiv.org/pdf/2104.04473.pdf).\n",
    "\n",
    "Let's execute the 2 following cells to estimate the training duration for the 18B parameters models trained on a dataset of $T$=300 billion tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "# Estimate the training time\n",
    "def estimate_days_needed(T , P , N ,PF):  \n",
    "    compute_sec=8*T*P/(N*PF*10e12)\n",
    "    # Convert compute seconds to days\n",
    "    to_days=round(compute_sec/(3600*24))\n",
    "    print(\"This language model will need {} days per epoch.\".format(colored(str(to_days),'blue', attrs=['bold'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tokens in the dataset\n",
    "T=300*10e09\n",
    "\n",
    "estimate_days_needed(T,P,number_GPUs,PF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 203 days, which is almost **7 months** required to train the 18B model on 16 GPUs (2 nodes) with a dataset of 300B tokens! \n",
    "\n",
    "In this case, scaling the number of nodes is unavoidable in order to train the model in a reasonable amount of time. \n",
    "\n",
    "For instance, consider a GPT-3 model with $P$=175 billion parameters trained on a dataset of $T$=300 billion tokens on $n$=1024 A100 GPUs. Using a batch size of 1536, we achieve $F$=140 teraFLOP/s per GPU. Thus, the time required to train this model is **34 days**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
